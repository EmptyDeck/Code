{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets have been loaded successfully. Let's examine the structure of each dataset:\n",
    "\n",
    "- `train.csv`: This dataset includes features such as `Store`, `DayOfWeek`, `Date`, `Sales`, `Customers`, `Open`, `Promo`, `StateHoliday`, and `SchoolHoliday`. The `Sales` column is our target variable that we want to predict.\n",
    "\n",
    "- `store.csv`: This dataset provides additional information about each store, including `StoreType`, `Assortment`, `CompetitionDistance`, `CompetitionOpenSince[Month/Year]`, `Promo2`, `Promo2Since[Year/Week]`, and `PromoInterval`.\n",
    "\n",
    "- `test.csv`: This dataset is similar to `train.csv` but without the `Sales` and `Customers` columns. We will use this dataset to make our sales predictions after training our model.\n",
    "\n",
    "The next step is to preprocess these datasets, including merging the train/test datasets with the store dataset, handling date features, dealing with missing values, encoding categorical variables, and normalizing numerical variables.\n",
    "\n",
    "Let's start with the merging process. We will merge the `train.csv` and `store.csv` datasets, as well as the `test.csv` and `store.csv` datasets. This will provide a complete set of features for each entry in the train and test datasets.\n",
    "\n",
    "The datasets have been merged successfully. Now we have a complete set of features for each record in both the training and test datasets.\n",
    "\n",
    "The next steps in preprocessing the data include:\n",
    "\n",
    "1. Handling date features: We'll convert the `Date` column into a format that can be understood by the model, likely extracting the year, month, and day as separate features.\n",
    "\n",
    "2. Handling missing values: We'll check for missing values in the datasets and decide how to handle them based on their nature and impact on the overall data.\n",
    "\n",
    "3. Encoding categorical variables: Variables like `StateHoliday`, `StoreType`, and `Assortment` are categorical and will be encoded to a format that can be used by our model.\n",
    "\n",
    "4. Normalizing numerical variables: Numerical variables are often normalized for better model performance.\n",
    "\n",
    "Let's proceed with these steps, starting with handling the date features.\n",
    "\n",
    "The date features have been successfully extracted and the original 'Date' column has been dropped from both datasets. The datasets now include 'Year', 'Month', and 'Day' columns.\n",
    "\n",
    "The next step is to handle missing values. Let's check if there are any missing values in our datasets and decide how to handle them. If a column has a small amount of missing values, we might fill them using a method such as forward fill, backward fill, or by using the mean or median of the column. If a column has a significant number of missing values, we may consider dropping the column, particularly if the column is not critical. Let's check for missing values.\n",
    "\n",
    "We have several columns with missing values in both the train and test datasets. Here is a brief description of what each of these columns represents:\n",
    "\n",
    "- `CompetitionDistance`: Distance in meters to the nearest competitor store. \n",
    "- `CompetitionOpenSinceMonth`: Gives the approximate month of the time the nearest competitor was opened.\n",
    "- `CompetitionOpenSinceYear`: Gives the approximate year of the time the nearest competitor was opened.\n",
    "- `Promo2SinceWeek`: Describes the calendar week when the store started participating in Promo2.\n",
    "- `Promo2SinceYear`: Describes the year when the store started participating in Promo2.\n",
    "- `PromoInterval`: Describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew.\n",
    "\n",
    "Since `CompetitionDistance` has relatively fewer missing values, we can fill these with the median distance. \n",
    "\n",
    "For the `CompetitionOpenSince[Month/Year]` columns, we can fill the missing values with the most frequent values, as these columns represent a specific time period and using median or mean might not make sense in this context.\n",
    "\n",
    "For the `Promo2Since[Week/Year]` and `PromoInterval` columns, the missing values likely indicate that the store is not participating in the ongoing promotion Promo2. Therefore, we can fill these missing values with zeros.\n",
    "\n",
    "The 'Open' column in the test dataset has 11 missing values. Since this column indicates whether a store is open or closed, we can assume that if it's not specified, the store is open, and thus fill these missing values with 1.\n",
    "\n",
    "Let's implement these missing value handling strategies.\n",
    "\n",
    "All missing values have been successfully handled in both the train and test datasets. \n",
    "\n",
    "The next step in the preprocessing pipeline is to encode the categorical variables. This involves converting categories into a format that can be used by our model. In our datasets, `StateHoliday`, `StoreType`, and `Assortment` are categorical variables. We'll use one-hot encoding, which creates new columns indicating the presence (or absence) of each possible value in the original data.\n",
    "\n",
    "Additionally, `PromoInterval` is a categorical variable, but it's a bit different because it contains comma-separated values. We'll handle this separately by creating separate binary columns for each possible month in the `PromoInterval`.\n",
    "\n",
    "Let's proceed with encoding these categorical variables.\n",
    "\n",
    "The categorical variables have been successfully encoded. The `StateHoliday`, `StoreType`, and `Assortment` columns have been replaced with their one-hot encoded counterparts. The `PromoInterval` column has been replaced with binary columns representing each possible month.\n",
    "\n",
    "The final step in preprocessing is to normalize the numerical variables. Normalizing these variables often leads to better performance because it ensures that all features have a similar scale, which is particularly important for models that use distance-based metrics.\n",
    "\n",
    "In our datasets, `CompetitionDistance`, `CompetitionOpenSinceMonth`, `CompetitionOpenSinceYear`, `Promo2SinceWeek`, `Promo2SinceYear`, `Year`, `Month`, and `Day` are numerical variables. We'll use the Min-Max normalization method, which scales each feature to a fixed range between 0 and 1.\n",
    "\n",
    "Let's proceed with this step.\n",
    "\n",
    "The numerical variables have been successfully normalized. All numerical features now have a similar scale, ranging from 0 to 1.\n",
    "\n",
    "Now that all preprocessing steps have been completed, we can proceed to building our deep learning model. We'll use TensorFlow's Keras API to create a simple fully connected neural network (also known as a multilayer perceptron).\n",
    "\n",
    "Before we can train our model, we need to do the following:\n",
    "\n",
    "1. Separate our target variable (`Sales`) from the rest of the training dataset.\n",
    "2. Ensure that the train and test datasets have the same columns. Any extra columns in the train dataset that are not present in the test dataset need to be dropped.\n",
    "\n",
    "Let's carry out these steps next.\n",
    "\n",
    "The target variable (`Sales`) has been separated from the rest of the training dataset and the `Customers` column has been dropped. Also, the train and test datasets now have the same columns. Here are the shapes of the datasets:\n",
    "\n",
    "- Training dataset (`X_train`): 1,017,209 rows, 38 columns\n",
    "- Target variable (`y_train`): 1,017,209 values\n",
    "- Test dataset (`test_df`): 41,088 rows, 38 columns\n",
    "\n",
    "Now, let's proceed with building the deep learning model. We'll use a simple fully connected neural network (also known as a multilayer perceptron) with a couple of hidden layers. We'll use the mean squared logarithmic error as the loss function, which is commonly used for regression problems with large range of outputs, such as this one. We'll use the Adam optimizer and track the mean absolute error metric during training. \n",
    "\n",
    "Let's build and compile the model.\n",
    "\n",
    "I'm sorry for the confusion, but it appears that TensorFlow, the library required to create deep learning models, is currently not available in this environment. My capabilities in this context are based on the GPT-3 model trained by OpenAI and I'm able to guide you through code, but the actual execution environment here doesn't support TensorFlow as of my training cut-off in September 2021.\n",
    "\n",
    "Although I can't run the model here, I can provide you with the steps and code to run on your local machine.\n",
    "\n",
    "Here is the code to define and train a simple neural network model using TensorFlow and Keras. You can run this code in your own Python environment, assuming you have the necessary libraries installed.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_logarithmic_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=256, validation_split=0.2)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "predictions = model.predict(test_df)\n",
    "\n",
    "# Create a submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_df['Id'],\n",
    "    'Sales': predictions.flatten()\n",
    "})\n",
    "\n",
    "# Save the submission dataframe to a csv file\n",
    "submission_df.to_csv('sample_submission.csv', index=False)\n",
    "```\n",
    "\n",
    "This code will create a neural network with two hidden layers, train it on your training data, and make predictions on your test data. The predictions are then saved to a CSV file for submission.\n",
    "\n",
    "Remember to adjust the number of epochs, batch size, and model architecture according to your needs. You might also want to add callbacks like `ModelCheckpoint` and `EarlyStopping` for better model training.\n",
    "\n",
    "Let me know if you need further assistance or explanation on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sb/2gzn9_qx589_ft0jp8q6nbbc0000gn/T/ipykernel_98518/1301930319.py:5: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv('train.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   Store  DayOfWeek        Date  Sales  Customers  Open  Promo StateHoliday  \\\n",
       " 0      1          5  2015-07-31   5263        555     1      1            0   \n",
       " 1      2          5  2015-07-31   6064        625     1      1            0   \n",
       " 2      3          5  2015-07-31   8314        821     1      1            0   \n",
       " 3      4          5  2015-07-31  13995       1498     1      1            0   \n",
       " 4      5          5  2015-07-31   4822        559     1      1            0   \n",
       " \n",
       "    SchoolHoliday  \n",
       " 0              1  \n",
       " 1              1  \n",
       " 2              1  \n",
       " 3              1  \n",
       " 4              1  ,\n",
       "    Store StoreType Assortment  CompetitionDistance  CompetitionOpenSinceMonth  \\\n",
       " 0      1         c          a               1270.0                        9.0   \n",
       " 1      2         a          a                570.0                       11.0   \n",
       " 2      3         a          a              14130.0                       12.0   \n",
       " 3      4         c          c                620.0                        9.0   \n",
       " 4      5         a          a              29910.0                        4.0   \n",
       " \n",
       "    CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  \\\n",
       " 0                    2008.0       0              NaN              NaN   \n",
       " 1                    2007.0       1             13.0           2010.0   \n",
       " 2                    2006.0       1             14.0           2011.0   \n",
       " 3                    2009.0       0              NaN              NaN   \n",
       " 4                    2015.0       0              NaN              NaN   \n",
       " \n",
       "      PromoInterval  \n",
       " 0              NaN  \n",
       " 1  Jan,Apr,Jul,Oct  \n",
       " 2  Jan,Apr,Jul,Oct  \n",
       " 3              NaN  \n",
       " 4              NaN  ,\n",
       "    Id  Store  DayOfWeek        Date  Open  Promo StateHoliday  SchoolHoliday\n",
       " 0   1      1          4  2015-09-17   1.0      1            0              0\n",
       " 1   2      3          4  2015-09-17   1.0      1            0              0\n",
       " 2   3      7          4  2015-09-17   1.0      1            0              0\n",
       " 3   4      8          4  2015-09-17   1.0      1            0              0\n",
       " 4   5      9          4  2015-09-17   1.0      1            0              0)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start by loading the datasets\n",
    "import pandas as pd\n",
    "\n",
    "# Load train data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "#train_df = pd.read_csv('/mnt/data/train.csv')\n",
    "\n",
    "# Load store data\n",
    "store_df = pd.read_csv('store.csv')\n",
    "#store_df = pd.read_csv('/mnt/data/store.csv')\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv('test.csv')\n",
    "#test_df = pd.read_csv('/mnt/data/test.csv')\n",
    "\n",
    "# Display the first few rows of each dataframe\n",
    "train_df.head(), store_df.head(), test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Store  DayOfWeek        Date  Sales  Customers  Open  Promo StateHoliday  \\\n",
       " 0      1          5  2015-07-31   5263        555     1      1            0   \n",
       " 1      2          5  2015-07-31   6064        625     1      1            0   \n",
       " 2      3          5  2015-07-31   8314        821     1      1            0   \n",
       " 3      4          5  2015-07-31  13995       1498     1      1            0   \n",
       " 4      5          5  2015-07-31   4822        559     1      1            0   \n",
       " \n",
       "    SchoolHoliday StoreType Assortment  CompetitionDistance  \\\n",
       " 0              1         c          a               1270.0   \n",
       " 1              1         a          a                570.0   \n",
       " 2              1         a          a              14130.0   \n",
       " 3              1         c          c                620.0   \n",
       " 4              1         a          a              29910.0   \n",
       " \n",
       "    CompetitionOpenSinceMonth  CompetitionOpenSinceYear  Promo2  \\\n",
       " 0                        9.0                    2008.0       0   \n",
       " 1                       11.0                    2007.0       1   \n",
       " 2                       12.0                    2006.0       1   \n",
       " 3                        9.0                    2009.0       0   \n",
       " 4                        4.0                    2015.0       0   \n",
       " \n",
       "    Promo2SinceWeek  Promo2SinceYear    PromoInterval  \n",
       " 0              NaN              NaN              NaN  \n",
       " 1             13.0           2010.0  Jan,Apr,Jul,Oct  \n",
       " 2             14.0           2011.0  Jan,Apr,Jul,Oct  \n",
       " 3              NaN              NaN              NaN  \n",
       " 4              NaN              NaN              NaN  ,\n",
       "    Id  Store  DayOfWeek        Date  Open  Promo StateHoliday  SchoolHoliday  \\\n",
       " 0   1      1          4  2015-09-17   1.0      1            0              0   \n",
       " 1   2      3          4  2015-09-17   1.0      1            0              0   \n",
       " 2   3      7          4  2015-09-17   1.0      1            0              0   \n",
       " 3   4      8          4  2015-09-17   1.0      1            0              0   \n",
       " 4   5      9          4  2015-09-17   1.0      1            0              0   \n",
       " \n",
       "   StoreType Assortment  CompetitionDistance  CompetitionOpenSinceMonth  \\\n",
       " 0         c          a               1270.0                        9.0   \n",
       " 1         a          a              14130.0                       12.0   \n",
       " 2         a          c              24000.0                        4.0   \n",
       " 3         a          a               7520.0                       10.0   \n",
       " 4         a          c               2030.0                        8.0   \n",
       " \n",
       "    CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  \\\n",
       " 0                    2008.0       0              NaN              NaN   \n",
       " 1                    2006.0       1             14.0           2011.0   \n",
       " 2                    2013.0       0              NaN              NaN   \n",
       " 3                    2014.0       0              NaN              NaN   \n",
       " 4                    2000.0       0              NaN              NaN   \n",
       " \n",
       "      PromoInterval  \n",
       " 0              NaN  \n",
       " 1  Jan,Apr,Jul,Oct  \n",
       " 2              NaN  \n",
       " 3              NaN  \n",
       " 4              NaN  )"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the train and store dataframes\n",
    "train_df = pd.merge(train_df, store_df, on='Store', how='left')\n",
    "\n",
    "# Merge the test and store dataframes\n",
    "test_df = pd.merge(test_df, store_df, on='Store', how='left')\n",
    "\n",
    "# Display the first few rows of each merged dataframe\n",
    "train_df.head(), test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Store  DayOfWeek  Sales  Customers  Open  Promo StateHoliday  \\\n",
       " 0      1          5   5263        555     1      1            0   \n",
       " 1      2          5   6064        625     1      1            0   \n",
       " 2      3          5   8314        821     1      1            0   \n",
       " 3      4          5  13995       1498     1      1            0   \n",
       " 4      5          5   4822        559     1      1            0   \n",
       " \n",
       "    SchoolHoliday StoreType Assortment  CompetitionDistance  \\\n",
       " 0              1         c          a               1270.0   \n",
       " 1              1         a          a                570.0   \n",
       " 2              1         a          a              14130.0   \n",
       " 3              1         c          c                620.0   \n",
       " 4              1         a          a              29910.0   \n",
       " \n",
       "    CompetitionOpenSinceMonth  CompetitionOpenSinceYear  Promo2  \\\n",
       " 0                        9.0                    2008.0       0   \n",
       " 1                       11.0                    2007.0       1   \n",
       " 2                       12.0                    2006.0       1   \n",
       " 3                        9.0                    2009.0       0   \n",
       " 4                        4.0                    2015.0       0   \n",
       " \n",
       "    Promo2SinceWeek  Promo2SinceYear    PromoInterval  Year  Month  Day  \n",
       " 0              NaN              NaN              NaN  2015      7   31  \n",
       " 1             13.0           2010.0  Jan,Apr,Jul,Oct  2015      7   31  \n",
       " 2             14.0           2011.0  Jan,Apr,Jul,Oct  2015      7   31  \n",
       " 3              NaN              NaN              NaN  2015      7   31  \n",
       " 4              NaN              NaN              NaN  2015      7   31  ,\n",
       "    Id  Store  DayOfWeek  Open  Promo StateHoliday  SchoolHoliday StoreType  \\\n",
       " 0   1      1          4   1.0      1            0              0         c   \n",
       " 1   2      3          4   1.0      1            0              0         a   \n",
       " 2   3      7          4   1.0      1            0              0         a   \n",
       " 3   4      8          4   1.0      1            0              0         a   \n",
       " 4   5      9          4   1.0      1            0              0         a   \n",
       " \n",
       "   Assortment  CompetitionDistance  CompetitionOpenSinceMonth  \\\n",
       " 0          a               1270.0                        9.0   \n",
       " 1          a              14130.0                       12.0   \n",
       " 2          c              24000.0                        4.0   \n",
       " 3          a               7520.0                       10.0   \n",
       " 4          c               2030.0                        8.0   \n",
       " \n",
       "    CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  \\\n",
       " 0                    2008.0       0              NaN              NaN   \n",
       " 1                    2006.0       1             14.0           2011.0   \n",
       " 2                    2013.0       0              NaN              NaN   \n",
       " 3                    2014.0       0              NaN              NaN   \n",
       " 4                    2000.0       0              NaN              NaN   \n",
       " \n",
       "      PromoInterval  Year  Month  Day  \n",
       " 0              NaN  2015      9   17  \n",
       " 1  Jan,Apr,Jul,Oct  2015      9   17  \n",
       " 2              NaN  2015      9   17  \n",
       " 3              NaN  2015      9   17  \n",
       " 4              NaN  2015      9   17  )"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'Date' column to datetime format for both train and test dataframes\n",
    "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
    "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
    "\n",
    "# Extract year, month, and day as separate features\n",
    "train_df['Year'] = train_df['Date'].dt.year\n",
    "train_df['Month'] = train_df['Date'].dt.month\n",
    "train_df['Day'] = train_df['Date'].dt.day\n",
    "\n",
    "test_df['Year'] = test_df['Date'].dt.year\n",
    "test_df['Month'] = test_df['Date'].dt.month\n",
    "test_df['Day'] = test_df['Date'].dt.day\n",
    "\n",
    "# Now that we've extracted the necessary information from the 'Date' column, we can drop it\n",
    "train_df.drop('Date', axis=1, inplace=True)\n",
    "test_df.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "# Check the updated dataframes\n",
    "train_df.head(), test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CompetitionDistance            2642\n",
       " CompetitionOpenSinceMonth    323348\n",
       " CompetitionOpenSinceYear     323348\n",
       " Promo2SinceWeek              508031\n",
       " Promo2SinceYear              508031\n",
       " PromoInterval                508031\n",
       " dtype: int64,\n",
       " Open                            11\n",
       " CompetitionDistance             96\n",
       " CompetitionOpenSinceMonth    15216\n",
       " CompetitionOpenSinceYear     15216\n",
       " Promo2SinceWeek              17232\n",
       " Promo2SinceYear              17232\n",
       " PromoInterval                17232\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in train and test datasets\n",
    "train_missing = train_df.isnull().sum()\n",
    "test_missing = test_df.isnull().sum()\n",
    "\n",
    "train_missing[train_missing > 0], test_missing[test_missing > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Series([], dtype: int64), Series([], dtype: int64))"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill missing values\n",
    "\n",
    "# For 'CompetitionDistance', we fill missing values with the median distance\n",
    "train_df['CompetitionDistance'].fillna(train_df['CompetitionDistance'].median(), inplace=True)\n",
    "test_df['CompetitionDistance'].fillna(test_df['CompetitionDistance'].median(), inplace=True)\n",
    "\n",
    "# For 'CompetitionOpenSinceMonth' and 'CompetitionOpenSinceYear', we fill missing values with the most frequent value\n",
    "train_df['CompetitionOpenSinceMonth'].fillna(train_df['CompetitionOpenSinceMonth'].mode()[0], inplace=True)\n",
    "train_df['CompetitionOpenSinceYear'].fillna(train_df['CompetitionOpenSinceYear'].mode()[0], inplace=True)\n",
    "\n",
    "test_df['CompetitionOpenSinceMonth'].fillna(test_df['CompetitionOpenSinceMonth'].mode()[0], inplace=True)\n",
    "test_df['CompetitionOpenSinceYear'].fillna(test_df['CompetitionOpenSinceYear'].mode()[0], inplace=True)\n",
    "\n",
    "# For 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval', we fill missing values with zero\n",
    "promo_cols = ['Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
    "train_df[promo_cols] = train_df[promo_cols].fillna(0)\n",
    "test_df[promo_cols] = test_df[promo_cols].fillna(0)\n",
    "\n",
    "# For 'Open' column in test dataset, fill missing values with 1\n",
    "test_df['Open'].fillna(1, inplace=True)\n",
    "\n",
    "# Check if there are any missing values left\n",
    "train_missing = train_df.isnull().sum()\n",
    "test_missing = test_df.isnull().sum()\n",
    "\n",
    "train_missing[train_missing > 0], test_missing[test_missing > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'PromoInterval' column:\n",
      "[0 'Jan,Apr,Jul,Oct' 'Feb,May,Aug,Nov' 'Mar,Jun,Sept,Dec']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   Store  DayOfWeek  Sales  Customers  Open  Promo  SchoolHoliday  \\\n",
       " 0      1          5   5263        555     1      1              1   \n",
       " 1      2          5   6064        625     1      1              1   \n",
       " 2      3          5   8314        821     1      1              1   \n",
       " 3      4          5  13995       1498     1      1              1   \n",
       " 4      5          5   4822        559     1      1              1   \n",
       " \n",
       "    CompetitionDistance  CompetitionOpenSinceMonth  CompetitionOpenSinceYear  \\\n",
       " 0               1270.0                        9.0                    2008.0   \n",
       " 1                570.0                       11.0                    2007.0   \n",
       " 2              14130.0                       12.0                    2006.0   \n",
       " 3                620.0                        9.0                    2009.0   \n",
       " 4              29910.0                        4.0                    2015.0   \n",
       " \n",
       "    ...  PromoInterval_Mar  PromoInterval_Apr  PromoInterval_May  \\\n",
       " 0  ...                  0                  0                  0   \n",
       " 1  ...                  0                  1                  0   \n",
       " 2  ...                  0                  1                  0   \n",
       " 3  ...                  0                  0                  0   \n",
       " 4  ...                  0                  0                  0   \n",
       " \n",
       "    PromoInterval_Jun  PromoInterval_Jul  PromoInterval_Aug  PromoInterval_Sep  \\\n",
       " 0                  0                  0                  0                  0   \n",
       " 1                  0                  1                  0                  0   \n",
       " 2                  0                  1                  0                  0   \n",
       " 3                  0                  0                  0                  0   \n",
       " 4                  0                  0                  0                  0   \n",
       " \n",
       "    PromoInterval_Oct  PromoInterval_Nov  PromoInterval_Dec  \n",
       " 0                  0                  0                  0  \n",
       " 1                  1                  0                  0  \n",
       " 2                  1                  0                  0  \n",
       " 3                  0                  0                  0  \n",
       " 4                  0                  0                  0  \n",
       " \n",
       " [5 rows x 40 columns],\n",
       "    Id  Store  DayOfWeek  Open  Promo  SchoolHoliday  CompetitionDistance  \\\n",
       " 0   1      1          4   1.0      1              0               1270.0   \n",
       " 1   2      3          4   1.0      1              0              14130.0   \n",
       " 2   3      7          4   1.0      1              0              24000.0   \n",
       " 3   4      8          4   1.0      1              0               7520.0   \n",
       " 4   5      9          4   1.0      1              0               2030.0   \n",
       " \n",
       "    CompetitionOpenSinceMonth  CompetitionOpenSinceYear  Promo2  ...  \\\n",
       " 0                        9.0                    2008.0       0  ...   \n",
       " 1                       12.0                    2006.0       1  ...   \n",
       " 2                        4.0                    2013.0       0  ...   \n",
       " 3                       10.0                    2014.0       0  ...   \n",
       " 4                        8.0                    2000.0       0  ...   \n",
       " \n",
       "    PromoInterval_Mar  PromoInterval_Apr  PromoInterval_May  PromoInterval_Jun  \\\n",
       " 0                  0                  0                  0                  0   \n",
       " 1                  0                  1                  0                  0   \n",
       " 2                  0                  0                  0                  0   \n",
       " 3                  0                  0                  0                  0   \n",
       " 4                  0                  0                  0                  0   \n",
       " \n",
       "    PromoInterval_Jul  PromoInterval_Aug  PromoInterval_Sep  PromoInterval_Oct  \\\n",
       " 0                  0                  0                  0                  0   \n",
       " 1                  1                  0                  0                  1   \n",
       " 2                  0                  0                  0                  0   \n",
       " 3                  0                  0                  0                  0   \n",
       " 4                  0                  0                  0                  0   \n",
       " \n",
       "    PromoInterval_Nov  PromoInterval_Dec  \n",
       " 0                  0                  0  \n",
       " 1                  0                  0  \n",
       " 2                  0                  0  \n",
       " 3                  0                  0  \n",
       " 4                  0                  0  \n",
       " \n",
       " [5 rows x 36 columns])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode 'StateHoliday', 'StoreType', 'Assortment' columns\n",
    "train_df = pd.get_dummies(train_df, columns=['StateHoliday', 'StoreType', 'Assortment'])\n",
    "test_df = pd.get_dummies(test_df, columns=['StateHoliday', 'StoreType', 'Assortment'])\n",
    "\n",
    "# Handle 'PromoInterval' column\n",
    "# First, let's see what unique values it has\n",
    "print(\"Unique values in 'PromoInterval' column:\")\n",
    "print(train_df['PromoInterval'].unique())\n",
    "\n",
    "# It seems like 'PromoInterval' contains either 0 (for no promotion) or a string with comma-separated months.\n",
    "# Let's create separate binary columns for each possible month in the 'PromoInterval'\n",
    "\n",
    "# Months\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "# For each month, create a new column in train and test dataframes\n",
    "for month in months:\n",
    "    train_df['PromoInterval_' + month] = train_df['PromoInterval'].apply(lambda x: 1 if month in str(x) else 0)\n",
    "    test_df['PromoInterval_' + month] = test_df['PromoInterval'].apply(lambda x: 1 if month in str(x) else 0)\n",
    "\n",
    "# Now that we've extracted the necessary information from the 'PromoInterval' column, we can drop it\n",
    "train_df.drop('PromoInterval', axis=1, inplace=True)\n",
    "test_df.drop('PromoInterval', axis=1, inplace=True)\n",
    "\n",
    "# Check the updated dataframes\n",
    "train_df.head(), test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Store  DayOfWeek  Sales  Customers  Open  Promo  SchoolHoliday  \\\n",
       " 0      1          5   5263        555     1      1              1   \n",
       " 1      2          5   6064        625     1      1              1   \n",
       " 2      3          5   8314        821     1      1              1   \n",
       " 3      4          5  13995       1498     1      1              1   \n",
       " 4      5          5   4822        559     1      1              1   \n",
       " \n",
       "    CompetitionDistance  CompetitionOpenSinceMonth  CompetitionOpenSinceYear  \\\n",
       " 0             0.016482                   0.727273                  0.939130   \n",
       " 1             0.007252                   0.909091                  0.930435   \n",
       " 2             0.186050                   1.000000                  0.921739   \n",
       " 3             0.007911                   0.727273                  0.947826   \n",
       " 4             0.394119                   0.272727                  1.000000   \n",
       " \n",
       "    ...  PromoInterval_Mar  PromoInterval_Apr  PromoInterval_May  \\\n",
       " 0  ...                  0                  0                  0   \n",
       " 1  ...                  0                  1                  0   \n",
       " 2  ...                  0                  1                  0   \n",
       " 3  ...                  0                  0                  0   \n",
       " 4  ...                  0                  0                  0   \n",
       " \n",
       "    PromoInterval_Jun  PromoInterval_Jul  PromoInterval_Aug  PromoInterval_Sep  \\\n",
       " 0                  0                  0                  0                  0   \n",
       " 1                  0                  1                  0                  0   \n",
       " 2                  0                  1                  0                  0   \n",
       " 3                  0                  0                  0                  0   \n",
       " 4                  0                  0                  0                  0   \n",
       " \n",
       "    PromoInterval_Oct  PromoInterval_Nov  PromoInterval_Dec  \n",
       " 0                  0                  0                  0  \n",
       " 1                  1                  0                  0  \n",
       " 2                  1                  0                  0  \n",
       " 3                  0                  0                  0  \n",
       " 4                  0                  0                  0  \n",
       " \n",
       " [5 rows x 40 columns],\n",
       "    Id  Store  DayOfWeek  Open  Promo  SchoolHoliday  CompetitionDistance  \\\n",
       " 0   1      1          4   1.0      1              0             0.016482   \n",
       " 1   2      3          4   1.0      1              0             0.186050   \n",
       " 2   3      7          4   1.0      1              0             0.316192   \n",
       " 3   4      8          4   1.0      1              0             0.098892   \n",
       " 4   5      9          4   1.0      1              0             0.026503   \n",
       " \n",
       "    CompetitionOpenSinceMonth  CompetitionOpenSinceYear  Promo2  ...  \\\n",
       " 0                   0.727273                  0.939130       0  ...   \n",
       " 1                   1.000000                  0.921739       1  ...   \n",
       " 2                   0.272727                  0.982609       0  ...   \n",
       " 3                   0.818182                  0.991304       0  ...   \n",
       " 4                   0.636364                  0.869565       0  ...   \n",
       " \n",
       "    PromoInterval_Mar  PromoInterval_Apr  PromoInterval_May  PromoInterval_Jun  \\\n",
       " 0                  0                  0                  0                  0   \n",
       " 1                  0                  1                  0                  0   \n",
       " 2                  0                  0                  0                  0   \n",
       " 3                  0                  0                  0                  0   \n",
       " 4                  0                  0                  0                  0   \n",
       " \n",
       "    PromoInterval_Jul  PromoInterval_Aug  PromoInterval_Sep  PromoInterval_Oct  \\\n",
       " 0                  0                  0                  0                  0   \n",
       " 1                  1                  0                  0                  1   \n",
       " 2                  0                  0                  0                  0   \n",
       " 3                  0                  0                  0                  0   \n",
       " 4                  0                  0                  0                  0   \n",
       " \n",
       "    PromoInterval_Nov  PromoInterval_Dec  \n",
       " 0                  0                  0  \n",
       " 1                  0                  0  \n",
       " 2                  0                  0  \n",
       " 3                  0                  0  \n",
       " 4                  0                  0  \n",
       " \n",
       " [5 rows x 36 columns])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize a scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# List of numerical columns\n",
    "num_cols = ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear', 'Year', 'Month', 'Day']\n",
    "\n",
    "# Apply the scaler to the numerical columns\n",
    "train_df[num_cols] = scaler.fit_transform(train_df[num_cols])\n",
    "test_df[num_cols] = scaler.transform(test_df[num_cols]) # Note: we use transform() here, not fit_transform()\n",
    "\n",
    "# Check the updated dataframes\n",
    "train_df.head(), test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1017209, 38), (1017209,), (41088, 38))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate target variable 'Sales' from the rest of the training dataset\n",
    "X_train = train_df.drop(['Sales', 'Customers'], axis=1) # We also drop 'Customers' as it's not present in the test dataset\n",
    "y_train = train_df['Sales']\n",
    "\n",
    "# Ensure that train and test datasets have the same columns\n",
    "missing_cols = set(X_train.columns) - set(test_df.columns)\n",
    "for c in missing_cols:\n",
    "    test_df[c] = 0\n",
    "test_df = test_df[X_train.columns]\n",
    "\n",
    "# Check the shapes of the datasets\n",
    "X_train.shape, y_train.shape, test_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Leaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Convert the dataframes into PyTorch tensors\n",
    "# X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1) # reshape to (n_samples, 1)\n",
    "# X_test_tensor = torch.tensor(test_df.values, dtype=torch.float32)\n",
    "\n",
    "# # Define the batch size\n",
    "# batch_size = 32\n",
    "\n",
    "# # Create data loaders\n",
    "# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Check if CUDA is available\n",
    "# use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# # Specify the device as either 'cuda' or 'cpu'\n",
    "# device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# # Remove the device specification for CUDA\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# # Redefine the model architecture\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(X_train_tensor.shape[1], 128),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(128, 64),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(64, 1)\n",
    "# )\n",
    "\n",
    "# # Move the model to the device\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Define the loss function\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# # Define the optimizer\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Define the number of training epochs\n",
    "# n_epochs = 1\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(n_epochs):\n",
    "#     model.train()\n",
    "#     train_loss = 0.0\n",
    "\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         # move tensors to the device\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "        \n",
    "#         # clear the gradients of all optimized variables\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # forward pass: compute predicted outputs by passing inputs to the model\n",
    "#         output = model(data)\n",
    "        \n",
    "#         # calculate the batch loss\n",
    "#         loss = criterion(output, target)\n",
    "        \n",
    "#         # backward pass: compute gradient of the loss with respect to model parameters\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # perform a single optimization step (parameter update)\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # update training loss\n",
    "#         train_loss += loss.item() * data.size(0)\n",
    "        \n",
    "#     # calculate average loss over an epoch\n",
    "#     train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "#     print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Convert the dataframes into PyTorch tensors\n",
    "# X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1) # reshape to (n_samples, 1)\n",
    "\n",
    "# # Split data into training and validation\n",
    "# dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# # Create data loaders\n",
    "# batch_size = 64\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Define the model architecture\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(X_train_tensor.shape[1], 128),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(128, 64),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(64, 1)\n",
    "# )\n",
    "\n",
    "# # Define the loss function\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# # Define the optimizer\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Store losses for plot\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# # Placeholder for best validation loss\n",
    "# best_val_loss = float('inf')\n",
    "# NB_EPOCH = 1000\n",
    "# # Training loop\n",
    "# for epoch in range(NB_EPOCH):\n",
    "#     model.train()\n",
    "#     train_loss = 0.0\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         # clear the gradients of all optimized variables\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # forward pass: compute predicted outputs by passing inputs to the model\n",
    "#         output = model(data)\n",
    "        \n",
    "#         # calculate the batch loss\n",
    "#         loss = criterion(output, target)\n",
    "        \n",
    "#         # backward pass: compute gradient of the loss with respect to model parameters\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # perform a single optimization step (parameter update)\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # update training loss\n",
    "#         train_loss += loss.item() * data.size(0)\n",
    "\n",
    "#     # calculate average loss over an epoch\n",
    "#     train_loss = train_loss/len(train_loader.dataset)\n",
    "#     train_losses.append(train_loss)\n",
    "\n",
    "#     # Evaluate on validation data\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in val_loader:\n",
    "#             output = model(data)\n",
    "#             loss = criterion(output, target)\n",
    "#             val_loss += loss.item() * data.size(0)\n",
    "#     val_loss = val_loss/len(val_loader.dataset)\n",
    "#     val_losses.append(val_loss)\n",
    "\n",
    "#     print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss, val_loss))\n",
    "\n",
    "#     # save model if validation loss has decreased\n",
    "#     if val_loss < best_val_loss:\n",
    "#         print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(best_val_loss, val_loss))\n",
    "#         torch.save(model.state_dict(), 'model.pt')\n",
    "#         best_val_loss = val_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot training and validation losses\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(train_losses, label='Training loss')\n",
    "# plt.plot(val_losses, label='Validation loss')\n",
    "# plt.title('Training and Validation Losses')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Set device to CPU\n",
    "# device = torch.device('mps')\n",
    "\n",
    "# # Convert the dataframes into PyTorch tensors\n",
    "# X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "# y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device) # reshape to (n_samples, 1)\n",
    "\n",
    "# # Split data into training and validation\n",
    "# dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# # Create data loaders\n",
    "# batch_size = 64\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Define the model architecture\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(X_train_tensor.shape[1], 128),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(128, 64),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(64, 1)\n",
    "# ).to(device) # move the model to the device\n",
    "\n",
    "# # Define the loss function\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# # Define the optimizer\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Store losses for plot\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# # Placeholder for best validation loss\n",
    "# best_val_loss = float('inf')\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(3):\n",
    "#     model.train()\n",
    "#     train_loss = 0.0\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         # clear the gradients of all optimized variables\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # forward pass: compute predicted outputs by passing inputs to the model\n",
    "#         output = model(data)\n",
    "        \n",
    "#         # calculate the batch loss\n",
    "#         loss = criterion(output, target)\n",
    "        \n",
    "#         # backward pass: compute gradient of the loss with respect to model parameters\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # perform a single optimization step (parameter update)\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # update training loss\n",
    "#         train_loss += loss.item() * data.size(0)\n",
    "\n",
    "#     # calculate average loss over an epoch\n",
    "#     train_loss = train_loss/len(train_loader.dataset)\n",
    "#     train_losses.append(train_loss)\n",
    "\n",
    "#     # Evaluate on validation data\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in val_loader:\n",
    "#             output = model(data)\n",
    "#             loss = criterion(output, target)\n",
    "#             val_loss += loss.item() * data.size(0)\n",
    "#     val_loss = val_loss/len(val_loader.dataset)\n",
    "#     val_losses.append(val_loss)\n",
    "\n",
    "#     print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss, val_loss))\n",
    "\n",
    "#     # save model if validation loss has decreased\n",
    "#     if val_loss < best_val_loss:\n",
    "#         print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(best_val_loss, val_loss))\n",
    "#         torch.save(model.state_dict(), 'model.pt')\n",
    "#         best_val_loss = val_loss\n",
    "\n",
    "# # plot training and validation losses\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(train_losses, label='Training loss')\n",
    "# plt.plot(val_losses, label='Validation loss')\n",
    "# plt.title('Training and Validation Losses')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the best model\n",
    "# model.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "# # Switch model to the evaluation mode\n",
    "# model.eval()\n",
    "\n",
    "# # Make predictions on the test data\n",
    "# with torch.no_grad():\n",
    "#     predictions = model(X_test_tensor).numpy()\n",
    "\n",
    "# # Create a submission dataframe\n",
    "# submission_df = pd.DataFrame({'Id': test_df.index + 1, 'Sales': predictions.flatten()})\n",
    "\n",
    "# # Save the submission dataframe as a csv file\n",
    "# submission_df.to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple device detected\n",
      "Activating Apple Silicon GPU\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "def GPU():\n",
    "    if torch.cuda.is_available() == True:\n",
    "        device = 'cuda'\n",
    "        templist = [1, 2, 3]\n",
    "        templist = torch.FloatTensor(templist).to(device)\n",
    "        print(\"Cuda torch working : \", end=\"\")\n",
    "        print(templist.is_cuda)\n",
    "        print(\"current device no. : \", end=\"\")\n",
    "        print(torch.cuda.current_device())\n",
    "        print(\"GPU device count : \", end=\"\")\n",
    "        print(torch.cuda.device_count())\n",
    "        print(\"GPU name : \", end=\"\")\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        print(\"device : \", device)\n",
    "        # Execute the nvidia-smi command using subprocess\n",
    "        try:\n",
    "            output = subprocess.check_output(['nvidia-smi']).decode('utf-8')\n",
    "            print(\"nvidia-smi output:\")\n",
    "            print(output)\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError) as e:\n",
    "            print(\"Error executing nvidia-smi command:\", str(e))\n",
    "    elif torch.backends.mps.is_available() == True:\n",
    "        print(\"Apple device detected\\nActivating Apple Silicon GPU\")\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        print(\"cant use gpu , activating cpu\")\n",
    "        device = 'cpu'\n",
    "\n",
    "    return device\n",
    "device = GPU()\n",
    "device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframes into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device) # reshape to (n_samples, 1)\n",
    "X_test_tensor = torch.tensor(test_df.values, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation\n",
    "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train_tensor.shape[1], 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1)\n",
    ").to(device) # move the model to the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FFN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.layer4 = nn.Linear(64, 32)\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(256)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(32)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batchnorm1(self.layer1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.batchnorm2(self.layer2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.batchnorm3(self.layer3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.batchnorm4(self.layer4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "model = FFN(X_train_tensor.shape[1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store losses for plot\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "# Placeholder for best validation loss\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop\n",
    "\n",
    "1. train\n",
    "2. calculate val loss while torch no grad\n",
    "3. if val loss is smaller , save model\n",
    "4. print train val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 23546138.909730 \tValidation Loss: 5963204.885063 \tRemaining Time: 303.48s\n",
      "Validation loss decreased (inf --> 5963204.885063). Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 6447005.410252 \tValidation Loss: 5642971.684215 \tRemaining Time: 274.94s\n",
      "Validation loss decreased (5963204.885063 --> 5642971.684215). Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 6277082.157996 \tValidation Loss: 5574030.597733 \tRemaining Time: 239.30s\n",
      "Validation loss decreased (5642971.684215 --> 5574030.597733). Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 6151766.951047 \tValidation Loss: 5526859.512367 \tRemaining Time: 204.44s\n",
      "Validation loss decreased (5574030.597733 --> 5526859.512367). Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 6039672.686556 \tValidation Loss: 5564565.218740 \tRemaining Time: 172.81s\n",
      "Epoch: 6 \tTraining Loss: 5922771.565764 \tValidation Loss: 5261788.702318 \tRemaining Time: 139.68s\n",
      "Validation loss decreased (5526859.512367 --> 5261788.702318). Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 5839094.290893 \tValidation Loss: 5117983.355354 \tRemaining Time: 104.36s\n",
      "Validation loss decreased (5261788.702318 --> 5117983.355354). Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 5764186.365439 \tValidation Loss: 4951882.642576 \tRemaining Time: 69.99s\n",
      "Validation loss decreased (5117983.355354 --> 4951882.642576). Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 5706315.165644 \tValidation Loss: 4831947.306869 \tRemaining Time: 35.58s\n",
      "Validation loss decreased (4951882.642576 --> 4831947.306869). Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 5625661.502115 \tValidation Loss: 4795413.721724 \tRemaining Time: 0.00s\n",
      "Validation loss decreased (4831947.306869 --> 4795413.721724). Saving model ...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()  # Start time\n",
    "NB_EPOCH  = 10\n",
    "for epoch in range(NB_EPOCH):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item() * data.size(0)\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    elapsed_time = time.time() - start_time  # Elapsed time\n",
    "    epochs_done = epoch + 1\n",
    "    epochs_remaining = NB_EPOCH - epochs_done\n",
    "    avg_time_per_epoch = elapsed_time / epochs_done\n",
    "    remaining_time = avg_time_per_epoch * epochs_remaining\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tRemaining Time: {:.2f}s'.format(epoch + 1, train_loss, val_loss, remaining_time))\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model ...'.format(best_val_loss, val_loss))\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        best_val_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss_graph.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "# Switch model to the evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make predictions on the test data\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor).numpy()\n",
    "\n",
    "# Create a submission dataframe\n",
    "submission_df = pd.DataFrame({'Id': test_df.index + 1, 'Sales': predictions.flatten()})\n",
    "\n",
    "# Save the submission dataframe as a csv file\n",
    "submission_df.to_csv('sample_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
