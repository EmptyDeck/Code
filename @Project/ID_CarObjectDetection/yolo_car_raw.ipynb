{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO CAR\n",
    "2023 08 16  \n",
    "1540i  \n",
    "let get init  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vid_4_18340.jpg',\n",
       " 'vid_4_2460.jpg',\n",
       " 'vid_4_10720.jpg',\n",
       " 'vid_4_14220.jpg',\n",
       " 'vid_4_6160.jpg']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()  #Here\n",
    "# List the contents of the \"data\" directory\n",
    "train_path = os.path.join(current_dir, \"training_images\")\n",
    "data_directory_contents = os.listdir(train_path)\n",
    "data_directory_contents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vid_5_30040.jpg',\n",
       " 'vid_5_27500.jpg',\n",
       " 'vid_5_28380.jpg',\n",
       " 'vid_5_26420.jpg',\n",
       " 'vid_5_31160.jpg']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()  #Here\n",
    "# List the contents of the \"data\" directory\n",
    "test_path = os.path.join(current_dir, \"testing_images\")\n",
    "data_directory_contents = os.listdir(test_path)\n",
    "data_directory_contents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the bounding boxes CSV file\n",
    "bounding_boxes_csv_path = os.path.join(current_dir, 'train_solution_bounding_boxes.csv')\n",
    "bounding_boxes_df = pd.read_csv(bounding_boxes_csv_path)\n",
    "\n",
    "# Display the first few rows of the CSV file\n",
    "bounding_boxes_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Define a function to visualize bounding boxes on images\n",
    "def visualize_bounding_boxes(image_path, bounding_boxes):\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    image = Image.open(image_path)\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    for box in bounding_boxes:\n",
    "        rect = patches.Rectangle(\n",
    "            (box['xmin'], box['ymin']), \n",
    "            box['xmax'] - box['xmin'], \n",
    "            box['ymax'] - box['ymin'], \n",
    "            linewidth=1, \n",
    "            edgecolor='r', \n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "# Randomly select an image from the training set\n",
    "random_image_name = random.choice(os.listdir(os.path.join(current_dir , 'training_images')))\n",
    "random_image_path = os.path.join(current_dir, 'training_images', random_image_name)\n",
    "\n",
    "# Get bounding boxes for the selected image\n",
    "bounding_boxes = bounding_boxes_df[bounding_boxes_df['image'] == random_image_name].to_dict('records')\n",
    "\n",
    "# Visualize the image with bounding boxes\n",
    "visualize_bounding_boxes(random_image_path, bounding_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vid_4_1000.jpg</td>\n",
       "      <td>281.259045</td>\n",
       "      <td>187.035071</td>\n",
       "      <td>327.727931</td>\n",
       "      <td>223.225547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vid_4_10000.jpg</td>\n",
       "      <td>15.163531</td>\n",
       "      <td>187.035071</td>\n",
       "      <td>120.329957</td>\n",
       "      <td>236.430180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vid_4_10040.jpg</td>\n",
       "      <td>239.192475</td>\n",
       "      <td>176.764801</td>\n",
       "      <td>361.968162</td>\n",
       "      <td>236.430180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vid_4_10020.jpg</td>\n",
       "      <td>496.483358</td>\n",
       "      <td>172.363256</td>\n",
       "      <td>630.020260</td>\n",
       "      <td>231.539575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vid_4_10060.jpg</td>\n",
       "      <td>16.630970</td>\n",
       "      <td>186.546010</td>\n",
       "      <td>132.558611</td>\n",
       "      <td>238.386422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image        xmin        ymin        xmax        ymax\n",
       "0   vid_4_1000.jpg  281.259045  187.035071  327.727931  223.225547\n",
       "1  vid_4_10000.jpg   15.163531  187.035071  120.329957  236.430180\n",
       "2  vid_4_10040.jpg  239.192475  176.764801  361.968162  236.430180\n",
       "3  vid_4_10020.jpg  496.483358  172.363256  630.020260  231.539575\n",
       "4  vid_4_10060.jpg   16.630970  186.546010  132.558611  238.386422"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the bounding boxes CSV file\n",
    "bounding_boxes_csv_path = os.path.join(current_dir, \"train_solution_bounding_boxes.csv\")\n",
    "bounding_boxes_df = pd.read_csv(bounding_boxes_csv_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "bounding_boxes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>yolo_format</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vid_4_1000.jpg</td>\n",
       "      <td>281.259045</td>\n",
       "      <td>187.035071</td>\n",
       "      <td>327.727931</td>\n",
       "      <td>223.225547</td>\n",
       "      <td>0 0.45043415340236687 0.539816602368421 0.0687...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vid_4_10000.jpg</td>\n",
       "      <td>15.163531</td>\n",
       "      <td>187.035071</td>\n",
       "      <td>120.329957</td>\n",
       "      <td>236.430180</td>\n",
       "      <td>0 0.10021707670857989 0.5571911197368421 0.155...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vid_4_10040.jpg</td>\n",
       "      <td>239.192475</td>\n",
       "      <td>176.764801</td>\n",
       "      <td>361.968162</td>\n",
       "      <td>236.430180</td>\n",
       "      <td>0 0.44464544142011836 0.5436776061842105 0.181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vid_4_10020.jpg</td>\n",
       "      <td>496.483358</td>\n",
       "      <td>172.363256</td>\n",
       "      <td>630.020260</td>\n",
       "      <td>231.539575</td>\n",
       "      <td>0 0.8332127352071006 0.5314510939473683 0.1975...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vid_4_10060.jpg</td>\n",
       "      <td>16.630970</td>\n",
       "      <td>186.546010</td>\n",
       "      <td>132.558611</td>\n",
       "      <td>238.386422</td>\n",
       "      <td>0 0.11034732271449704 0.5591216215789474 0.171...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image        xmin        ymin        xmax        ymax  \\\n",
       "0   vid_4_1000.jpg  281.259045  187.035071  327.727931  223.225547   \n",
       "1  vid_4_10000.jpg   15.163531  187.035071  120.329957  236.430180   \n",
       "2  vid_4_10040.jpg  239.192475  176.764801  361.968162  236.430180   \n",
       "3  vid_4_10020.jpg  496.483358  172.363256  630.020260  231.539575   \n",
       "4  vid_4_10060.jpg   16.630970  186.546010  132.558611  238.386422   \n",
       "\n",
       "                                         yolo_format  \n",
       "0  0 0.45043415340236687 0.539816602368421 0.0687...  \n",
       "1  0 0.10021707670857989 0.5571911197368421 0.155...  \n",
       "2  0 0.44464544142011836 0.5436776061842105 0.181...  \n",
       "3  0 0.8332127352071006 0.5314510939473683 0.1975...  \n",
       "4  0 0.11034732271449704 0.5591216215789474 0.171...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def convert_to_yolo_format(row, img_path):\n",
    "    # Open the image to get its width and height\n",
    "    with Image.open(img_path) as img:\n",
    "        img_width, img_height = img.size\n",
    "    \n",
    "    # Calculate center, width and height of the bounding box\n",
    "    center_x = (row['xmin'] + row['xmax']) / 2.0\n",
    "    center_y = (row['ymin'] + row['ymax']) / 2.0\n",
    "    width = row['xmax'] - row['xmin']\n",
    "    height = row['ymax'] - row['ymin']\n",
    "    \n",
    "    # Normalize the values\n",
    "    center_x /= img_width\n",
    "    center_y /= img_height\n",
    "    width /= img_width\n",
    "    height /= img_height\n",
    "    \n",
    "    return f\"0 {center_x} {center_y} {width} {height}\"\n",
    "\n",
    "# Apply the conversion to each row in the dataframe\n",
    "training_images_path = os.path.join(current_dir, \"training_images\")\n",
    "bounding_boxes_df['yolo_format'] = bounding_boxes_df.apply(lambda row: convert_to_yolo_format(row, os.path.join(training_images_path, row['image'])), axis=1)\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "bounding_boxes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vid_4_740.txt',\n",
       " 'vid_4_6260.txt',\n",
       " 'vid_4_13840.txt',\n",
       " 'vid_4_15000.txt',\n",
       " 'vid_4_21400.txt']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directory to save YOLO formatted labels\n",
    "labels_directory = os.path.join(current_dir, \"yolo_labels\")\n",
    "os.makedirs(labels_directory, exist_ok=True)\n",
    "\n",
    "for _, row in bounding_boxes_df.iterrows():\n",
    "    # Create a text file for each image with the corresponding YOLO format data\n",
    "    txt_filename = os.path.splitext(row['image'])[0] + '.txt'\n",
    "    txt_path = os.path.join(labels_directory, txt_filename)\n",
    "    with open(txt_path, 'w') as f:\n",
    "        f.write(row['yolo_format'] + \"\\n\")\n",
    "\n",
    "# Check the first few created files\n",
    "created_files = os.listdir(labels_directory)[:5]\n",
    "created_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/owo/HOUSE/@Code/@Project/ID_CarObjectDetection/dataset.data'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a names file for class names\n",
    "classes_names_path = os.path.join(current_dir, \"classes.names\")\n",
    "with open(classes_names_path, 'w') as f:\n",
    "    f.write(\"object\\n\")  # Only one class named \"object\"\n",
    "\n",
    "# Create a data file with paths to datasets and class names\n",
    "data_file_content = f\"\"\"\n",
    "classes = 1\n",
    "train = {os.path.join(current_dir, \"train.txt\")}\n",
    "valid = {os.path.join(current_dir, \"valid.txt\")}\n",
    "names = {classes_names_path}\n",
    "backup = backup/\n",
    "\"\"\"\n",
    "\n",
    "data_file_path = os.path.join(current_dir, \"dataset.data\")\n",
    "with open(data_file_path, 'w') as f:\n",
    "    f.write(data_file_content)\n",
    "\n",
    "data_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/owo/HOUSE/@Code/@Project/ID_CarObjectDetection/train.txt',\n",
       " '/Users/owo/HOUSE/@Code/@Project/ID_CarObjectDetection/valid.txt')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Get all training image paths\n",
    "all_image_paths = [os.path.join(training_images_path, img_name) for img_name in bounding_boxes_df['image'].tolist()]\n",
    "\n",
    "# Shuffle and split the dataset into training and validation sets\n",
    "random.shuffle(all_image_paths)\n",
    "split_point = int(0.9 * len(all_image_paths))\n",
    "train_images = all_image_paths[:split_point]\n",
    "valid_images = all_image_paths[split_point:]\n",
    "\n",
    "# Save the paths to train.txt and valid.txt files\n",
    "train_txt_path = os.path.join(current_dir, \"train.txt\")\n",
    "valid_txt_path = os.path.join(current_dir, \"valid.txt\")\n",
    "\n",
    "with open(train_txt_path, 'w') as f:\n",
    "    f.write(\"\\n\".join(train_images))\n",
    "\n",
    "with open(valid_txt_path, 'w') as f:\n",
    "    f.write(\"\\n\".join(valid_images))\n",
    "\n",
    "train_txt_path, valid_txt_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image size fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(676, 380)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the first image to get its dimensions\n",
    "with Image.open(all_image_paths[0]) as img:\n",
    "    img_width, img_height = img.size\n",
    "\n",
    "img_width, img_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(704, 384)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the network width and height as multiples of 32\n",
    "network_width = (img_width + 31) // 32 * 32\n",
    "network_height = (img_height + 31) // 32 * 32\n",
    "\n",
    "network_width, network_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/owo/HOUSE/@Code/@Project/ID_CarObjectDetection/yolov3_custom.cfg'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOLOv3 configuration for training\n",
    "yolov3_config = f\"\"\"\n",
    "[net]\n",
    "# Testing\n",
    "batch=4\n",
    "subdivisions=2\n",
    "# Training\n",
    "# batch=64\n",
    "# subdivisions=16\n",
    "width={network_width}\n",
    "height={network_height}\n",
    "channels=3\n",
    "momentum=0.9\n",
    "decay=0.0005\n",
    "angle=0\n",
    "saturation = 1.5\n",
    "exposure = 1.5\n",
    "hue=.1\n",
    "\n",
    "learning_rate=0.001\n",
    "burn_in=1000\n",
    "max_batches = 500200\n",
    "policy=steps\n",
    "steps=400000,450000\n",
    "scales=.1,.1\n",
    "\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=32\n",
    "size=3\n",
    "stride=1\n",
    "pad=1\n",
    "activation=leaky\n",
    "\n",
    "# ... (Many more layers)\n",
    "\n",
    "[region]\n",
    "anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
    "bias_match=1\n",
    "classes=1\n",
    "coords=4\n",
    "num=9\n",
    "jitter=.3\n",
    "ignore_thresh = .7\n",
    "truth_thresh = 1\n",
    "random=1\n",
    "\"\"\"\n",
    "\n",
    "# Save the configuration to a file\n",
    "cfg_path = os.path.join(current_dir, \"yolov3_custom.cfg\")\n",
    "with open(cfg_path, 'w') as f:\n",
    "    f.write(yolov3_config)\n",
    "\n",
    "cfg_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num class (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/owo/anaconda3/envs/torchenv/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/owo/anaconda3/envs/torchenv/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'YOLODataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/owo/anaconda3/envs/torchenv/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/owo/anaconda3/envs/torchenv/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'YOLODataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/owo/anaconda3/envs/torchenv/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/owo/anaconda3/envs/torchenv/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'YOLODataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/owo/anaconda3/envs/torchenv/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/owo/anaconda3/envs/torchenv/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'YOLODataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 49906, 49910) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_queue\u001b[39m.\u001b[39mget(timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m   1133\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll(timeout):\n\u001b[1;32m    114\u001b[0m     \u001b[39mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/multiprocessing/connection.py:256\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 256\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll(timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/multiprocessing/connection.py:423\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 423\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39m], timeout)\n\u001b[1;32m    424\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/multiprocessing/connection.py:930\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 930\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39mselect(timeout)\n\u001b[1;32m    931\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_selector\u001b[39m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[39m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     _error_if_any_worker_fails()\n\u001b[1;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m previous_handler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 49906) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m dataset \u001b[39m=\u001b[39m YOLODataset(train_txt_path, img_size\u001b[39m=\u001b[39m\u001b[39m704\u001b[39m)\n\u001b[1;32m     36\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(dataloader))\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_data()\n\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_get_data()\n\u001b[1;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1145\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1144\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1145\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[1;32m   1147\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 49906, 49910) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, list_path, img_size=416):\n",
    "        with open(list_path, 'r') as file:\n",
    "            self.img_files = file.readlines()\n",
    "        \n",
    "        self.label_files = [path.replace('images', 'labels').replace('.png', '.txt').replace('.jpg', '.txt') for path in self.img_files]\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_files[index % len(self.img_files)].rstrip()\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = img.resize((self.img_size, self.img_size))\n",
    "        img = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1)  # Convert to (C, H, W) format\n",
    "\n",
    "        label_path = self.label_files[index % len(self.img_files)].rstrip()\n",
    "        labels = torch.zeros((0, 5), dtype=torch.float32)  # Default empty label\n",
    "        if os.path.exists(label_path):\n",
    "            labels_data = [list(map(float, line.split())) for line in open(label_path)]\n",
    "            if labels_data:\n",
    "                labels = torch.tensor(labels_data, dtype=torch.float32)\n",
    "\n",
    "        return img, labels\n",
    "\n",
    "# Example usage\n",
    "dataset = YOLODataset(train_txt_path, img_size=704)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DarknetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DarknetBlock, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(out_channels, in_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.layer(x)\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    def __init__(self, num_blocks, num_classes):\n",
    "        super(Darknet, self).__init__()\n",
    "\n",
    "        def make_layers(in_channels, out_channels, num_blocks):\n",
    "            layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                      nn.BatchNorm2d(out_channels),\n",
    "                      nn.LeakyReLU(0.1)]\n",
    "            for _ in range(num_blocks):\n",
    "                layers.append(DarknetBlock(out_channels, out_channels*2))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        self.layer1 = make_layers(3, 32, num_blocks[0])\n",
    "        self.layer2 = make_layers(32, 64, num_blocks[1])\n",
    "        self.layer3 = make_layers(64, 128, num_blocks[2])\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Darknet([1, 2, 4], num_classes=4*(5+num_classes)).to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(YOLOv3, self).__init__()\n",
    "        \n",
    "        # Darknet-53 backbone\n",
    "        self.backbone = Darknet([1, 2, 8, 8, 4], num_classes)\n",
    "        \n",
    "        # Detection heads\n",
    "        self.head1 = self._make_head(128, num_classes)\n",
    "        self.head2 = self._make_head(256, num_classes)\n",
    "        self.head3 = self._make_head(512, num_classes)\n",
    "        \n",
    "    def _make_head(self, in_channels, num_classes):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(in_channels*2),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(in_channels*2, 3*(4 + 1 + num_classes), kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        \n",
    "        out1 = self.head1(x)\n",
    "        out2 = self.head2(x)\n",
    "        out3 = self.head3(x)\n",
    "        \n",
    "        return out1, out2, out3\n",
    "\n",
    "model = YOLOv3(num_classes=num_classes).to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOLoss(nn.Module):\n",
    "    def __init__(self, anchors, num_classes, img_dim):\n",
    "        super(YOLOLoss, self).__init__()\n",
    "        self.anchors = anchors\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.num_classes = num_classes\n",
    "        self.bbox_attrs = 5 + num_classes\n",
    "        self.img_dim = img_dim\n",
    "        self.ignore_thres = 0.5\n",
    "        self.lambda_coord = 1\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets):\n",
    "        nB = x.size(0)\n",
    "        nA = self.num_anchors\n",
    "        nC = self.num_classes\n",
    "        nG = x.size(2)\n",
    "        \n",
    "        stride = self.img_dim / nG\n",
    "\n",
    "        # Convert anchors to current grid size\n",
    "        scaled_anchors = [(a_w / stride, a_h / stride) for a_w, a_h in self.anchors]\n",
    "        scaled_anchors = torch.FloatTensor(scaled_anchors).to(device)\n",
    "\n",
    "        # Tensors for cuda support\n",
    "        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
    "        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
    "        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\n",
    "\n",
    "        prediction = (\n",
    "            x.view(nB, nA, self.bbox_attrs, nG, nG)\n",
    "            .permute(0, 1, 3, 4, 2)\n",
    "            .contiguous()\n",
    "        )\n",
    "\n",
    "        # Get outputs\n",
    "        x = torch.sigmoid(prediction[..., 0])  # Center x\n",
    "        y = torch.sigmoid(prediction[..., 1])  # Center y\n",
    "        w = prediction[..., 2]  # Width\n",
    "        h = prediction[..., 3]  # Height\n",
    "        pred_conf = torch.sigmoid(prediction[..., 4])  # Conf\n",
    "        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.\n",
    "\n",
    "        # Calculate offsets for each grid\n",
    "        grid_x = torch.arange(nG).repeat(nG, 1).view([1, 1, nG, nG]).type(FloatTensor)\n",
    "        grid_y = torch.arange(nG).repeat(nG, 1).t().view([1, 1, nG, nG]).type(FloatTensor)\n",
    "        scaled_anchors = FloatTensor(scaled_anchors)\n",
    "        anchor_w = scaled_anchors[:, 0:1].view((1, nA, 1, 1))\n",
    "        anchor_h = scaled_anchors[:, 1:2].view((1, nA, 1, 1))\n",
    "\n",
    "        # Add offset and scale with anchors\n",
    "        pred_boxes = FloatTensor(prediction[..., :4].shape)\n",
    "        pred_boxes[..., 0] = x.data + grid_x\n",
    "        pred_boxes[..., 1] = y.data + grid_y\n",
    "        pred_boxes[..., 2] = torch.exp(w.data) * anchor_w\n",
    "        pred_boxes[..., 3] = torch.exp(h.data) * anchor_h\n",
    "\n",
    "        # Calculate for all targets\n",
    "        obj_mask = ByteTensor(nB, nA, nG, nG).fill_(0)\n",
    "        noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(1)\n",
    "        class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "        iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "        tx = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "        ty = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "        tw = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "        th = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "        tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)\n",
    "\n",
    "        # Convert to position relative to box\n",
    "        target_boxes = targets[:, 2:6] * nG\n",
    "        gxy = target_boxes[:, :2]\n",
    "        gwh = target_boxes[:, 2:]\n",
    "        # Get anchors with best iou\n",
    "        ious = torch.stack([self.bbox_wh_iou(anchor, gwh) for anchor in scaled_anchors])\n",
    "        best_ious, best_n = ious.max(0)\n",
    "        # Separate target values\n",
    "        b, target_labels = targets[:, :2].long().t()\n",
    "        gx, gy = gxy.t()\n",
    "        gw, gh = gwh.t()\n",
    "        gi, gj = gxy.long().t()\n",
    "\n",
    "        # Set masks\n",
    "        obj_mask[b, best_n, gj, gi] = 1\n",
    "        noobj_mask[b, best_n, gj, gi] = 0\n",
    "\n",
    "        # Set noobj mask to zero where iou exceeds ignore threshold\n",
    "        for i, anchor_ious in enumerate(ious.t()):\n",
    "            noobj_mask[b[i], anchor_ious > self.ignore_thres, gj[i], gi[i]] = 0\n",
    "\n",
    "        # Coordinates\n",
    "        tx[b, best_n, gj, gi] = gx - gx.floor()\n",
    "        ty[b, best_n, gj, gi] = gy - gy.floor()\n",
    "        # Width and height\n",
    "        tw[b, best_n, gj, gi] = torch.log(gw / scaled_anchors[best_n][:, 0] + 1e-16)\n",
    "        th[b, best_n, gj, gi] = torch.log(gh / scaled_anchors[best_n][:, 1] + 1e-16)\n",
    "        # One-hot encoding of label\n",
    "        tcls[b, best_n, gj, gi, target_labels] = 1\n",
    "\n",
    "        # Compute some obj_mask values to avoid zero division\n",
    "        obj_mask = obj_mask.bool()\n",
    "        noobj_mask = noobj_mask.bool()\n",
    "\n",
    "        # Loss : Mask outputs to ignore non-existing objects (except with conf. loss)\n",
    "        # Continue the loss calculation\n",
    "        loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])\n",
    "        loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])\n",
    "        loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])\n",
    "        loss_h = self.mse_loss(h[obj_mask], th[obj_mask])\n",
    "        \n",
    "        loss_conf_obj = self.bce_loss(pred_conf[obj_mask], torch.ones_like(pred_conf[obj_mask]))\n",
    "        loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], torch.zeros_like(pred_conf[noobj_mask]))\n",
    "        loss_conf = loss_conf_obj + loss_conf_noobj\n",
    "        \n",
    "        loss_cls = self.ce_loss(pred_cls[obj_mask], torch.argmax(tcls[obj_mask], 1))\n",
    "        total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    def bbox_wh_iou(self, wh1, wh2):\n",
    "        wh2 = wh2.t()\n",
    "        w1, h1 = wh1[0], wh1[1]\n",
    "        w2, h2 = wh2[0], wh2[1]\n",
    "        inter_area = torch.min(w1, w2) * torch.min(h1, h2)\n",
    "        union_area = (w1 * h1 + 1e-16) + w2 * h2 - inter_area\n",
    "        return inter_area / union_area\n",
    "\n",
    "# Create the loss function\n",
    "yolo_loss = YOLOLoss(anchors, num_classes, 704).to(device)\n",
    "yolo_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from models import Darknet\n",
    "from utils.datasets import ListDataset\n",
    "from utils.utils import compute_loss\n",
    "\n",
    "# Configuration\n",
    "cfg = '/path/to/yolov3_custom.cfg'\n",
    "data_config = '/path/to/dataset.data'\n",
    "img_size = 704\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = Darknet(cfg, img_size=img_size).to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Get dataloader\n",
    "train_path = '/path/to/train.txt'\n",
    "dataset = ListDataset(train_path, img_size=img_size)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=dataset.collate_fn)\n",
    "import torch.optim as optim\n",
    "\n",
    "# Model\n",
    "model = YOLOv3(num_classes=num_classes).to(device)\n",
    "\n",
    "# Loss function\n",
    "anchors = [(10,13), (16,30), (33,23), (30,61), (62,45), (59,119), (116,90), (156,198), (373,326)]\n",
    "yolo_loss = YOLOLoss(anchors, num_classes, 704).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100\n",
    "print_every = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(dataloader):\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Loss\n",
    "        loss = sum([yolo_loss(output, targets) for output in outputs])\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Print updates\n",
    "    if (epoch+1) % print_every == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
