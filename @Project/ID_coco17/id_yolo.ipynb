{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['info', 'licenses', 'images', 'annotations', 'categories'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the instances_val2017.json file\n",
    "with open(\"annotations/instances_train2017.json\", \"r\") as file:\n",
    "    instances_data = json.load(file)\n",
    "\n",
    "# Display the keys in the loaded data\n",
    "instances_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay, lets get the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'person',\n",
       " 2: 'bicycle',\n",
       " 3: 'car',\n",
       " 4: 'motorcycle',\n",
       " 5: 'airplane',\n",
       " 6: 'bus',\n",
       " 7: 'train',\n",
       " 8: 'truck',\n",
       " 9: 'boat',\n",
       " 10: 'traffic light',\n",
       " 11: 'fire hydrant',\n",
       " 13: 'stop sign',\n",
       " 14: 'parking meter',\n",
       " 15: 'bench',\n",
       " 16: 'bird',\n",
       " 17: 'cat',\n",
       " 18: 'dog',\n",
       " 19: 'horse',\n",
       " 20: 'sheep',\n",
       " 21: 'cow',\n",
       " 22: 'elephant',\n",
       " 23: 'bear',\n",
       " 24: 'zebra',\n",
       " 25: 'giraffe',\n",
       " 27: 'backpack',\n",
       " 28: 'umbrella',\n",
       " 31: 'handbag',\n",
       " 32: 'tie',\n",
       " 33: 'suitcase',\n",
       " 34: 'frisbee',\n",
       " 35: 'skis',\n",
       " 36: 'snowboard',\n",
       " 37: 'sports ball',\n",
       " 38: 'kite',\n",
       " 39: 'baseball bat',\n",
       " 40: 'baseball glove',\n",
       " 41: 'skateboard',\n",
       " 42: 'surfboard',\n",
       " 43: 'tennis racket',\n",
       " 44: 'bottle',\n",
       " 46: 'wine glass',\n",
       " 47: 'cup',\n",
       " 48: 'fork',\n",
       " 49: 'knife',\n",
       " 50: 'spoon',\n",
       " 51: 'bowl',\n",
       " 52: 'banana',\n",
       " 53: 'apple',\n",
       " 54: 'sandwich',\n",
       " 55: 'orange',\n",
       " 56: 'broccoli',\n",
       " 57: 'carrot',\n",
       " 58: 'hot dog',\n",
       " 59: 'pizza',\n",
       " 60: 'donut',\n",
       " 61: 'cake',\n",
       " 62: 'chair',\n",
       " 63: 'couch',\n",
       " 64: 'potted plant',\n",
       " 65: 'bed',\n",
       " 67: 'dining table',\n",
       " 70: 'toilet',\n",
       " 72: 'tv',\n",
       " 73: 'laptop',\n",
       " 74: 'mouse',\n",
       " 75: 'remote',\n",
       " 76: 'keyboard',\n",
       " 77: 'cell phone',\n",
       " 78: 'microwave',\n",
       " 79: 'oven',\n",
       " 80: 'toaster',\n",
       " 81: 'sink',\n",
       " 82: 'refrigerator',\n",
       " 84: 'book',\n",
       " 85: 'clock',\n",
       " 86: 'vase',\n",
       " 87: 'scissors',\n",
       " 88: 'teddy bear',\n",
       " 89: 'hair drier',\n",
       " 90: 'toothbrush'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the categories from the data\n",
    "categories = instances_data[\"categories\"]\n",
    "\n",
    "# Create a mapping of category id to category name and index\n",
    "category_id_to_name = {cat[\"id\"]: cat[\"name\"] for cat in categories}\n",
    "# Using for to make numbers 0 to ~\n",
    "category_id_to_idx = {cat[\"id\"]: idx for idx, cat in enumerate(categories)}\n",
    "\n",
    "category_id_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0,\n",
       " 2: 1,\n",
       " 3: 2,\n",
       " 4: 3,\n",
       " 5: 4,\n",
       " 6: 5,\n",
       " 7: 6,\n",
       " 8: 7,\n",
       " 9: 8,\n",
       " 10: 9,\n",
       " 11: 10,\n",
       " 13: 11,\n",
       " 14: 12,\n",
       " 15: 13,\n",
       " 16: 14,\n",
       " 17: 15,\n",
       " 18: 16,\n",
       " 19: 17,\n",
       " 20: 18,\n",
       " 21: 19,\n",
       " 22: 20,\n",
       " 23: 21,\n",
       " 24: 22,\n",
       " 25: 23,\n",
       " 27: 24,\n",
       " 28: 25,\n",
       " 31: 26,\n",
       " 32: 27,\n",
       " 33: 28,\n",
       " 34: 29,\n",
       " 35: 30,\n",
       " 36: 31,\n",
       " 37: 32,\n",
       " 38: 33,\n",
       " 39: 34,\n",
       " 40: 35,\n",
       " 41: 36,\n",
       " 42: 37,\n",
       " 43: 38,\n",
       " 44: 39,\n",
       " 46: 40,\n",
       " 47: 41,\n",
       " 48: 42,\n",
       " 49: 43,\n",
       " 50: 44,\n",
       " 51: 45,\n",
       " 52: 46,\n",
       " 53: 47,\n",
       " 54: 48,\n",
       " 55: 49,\n",
       " 56: 50,\n",
       " 57: 51,\n",
       " 58: 52,\n",
       " 59: 53,\n",
       " 60: 54,\n",
       " 61: 55,\n",
       " 62: 56,\n",
       " 63: 57,\n",
       " 64: 58,\n",
       " 65: 59,\n",
       " 67: 60,\n",
       " 70: 61,\n",
       " 72: 62,\n",
       " 73: 63,\n",
       " 74: 64,\n",
       " 75: 65,\n",
       " 76: 66,\n",
       " 77: 67,\n",
       " 78: 68,\n",
       " 79: 69,\n",
       " 80: 70,\n",
       " 81: 71,\n",
       " 82: 72,\n",
       " 84: 73,\n",
       " 85: 74,\n",
       " 86: 75,\n",
       " 87: 76,\n",
       " 88: 77,\n",
       " 89: 78,\n",
       " 90: 79}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_id_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so the we have 80 labels (when you see close, some labels numbers skip).  \n",
    "now, lets get the image data and change it to yolo form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{391895: (640, 360),\n",
       " 522418: (640, 480),\n",
       " 184613: (500, 336),\n",
       " 318219: (556, 640),\n",
       " 554625: (426, 640),\n",
       " 574769: (480, 640),\n",
       " 60623: (640, 427),\n",
       " 309022: (640, 480),\n",
       " 5802: (640, 479),\n",
       " 222564: (640, 480),\n",
       " 118113: (480, 640),\n",
       " 193271: (480, 320),\n",
       " 224736: (640, 427),\n",
       " 483108: (428, 640),\n",
       " 403013: (301, 450),\n",
       " 374628: (640, 326),\n",
       " 328757: (500, 332),\n",
       " 384213: (375, 500),\n",
       " 293802: (425, 640),\n",
       " 86408: (640, 427)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mapping of image id to its width and height\n",
    "image_id_to_dimensions = {img[\"id\"]: (img[\"width\"], img[\"height\"]) for img in instances_data[\"images\"]}\n",
    "\n",
    "# Display a sample of the mapping\n",
    "sample_dimensions = {k: image_id_to_dimensions[k] for k in list(image_id_to_dimensions)[:20]}\n",
    "sample_dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so the image number is 397133 and the image size is 640,427. Lets see if thats right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File train2017/000000391895.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000522418.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000184613.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000318219.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000554625.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000574769.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000060623.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000309022.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000005802.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000222564.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000118113.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000193271.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000224736.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000483108.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000403013.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000374628.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000328757.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000384213.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000293802.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000086408.jpg not found. Please ensure the image exists in the specified directory.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# ÏòàÏÉÅÎêòÎäî Ïù¥ÎØ∏ÏßÄ ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú (Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ ÌååÏùºÏù¥ ÏûàÎäî Í≤ΩÎ°úÎ°ú Î≥ÄÍ≤ΩÌï¥Ïïº Ìï©ÎãàÎã§.)\n",
    "image_dir = \"train2017\"\n",
    "\n",
    "# Ï£ºÏñ¥ÏßÑ Ïù¥ÎØ∏ÏßÄ ID Î¶¨Ïä§Ìä∏Î•º Í∏∞Î∞òÏúºÎ°ú Ïù¥ÎØ∏ÏßÄÎ•º Î°úÎìúÌïòÍ≥† Ï∂úÎ†•Ìï©ÎãàÎã§.\n",
    "for image_id in sample_dimensions.keys():\n",
    "    image_path = f\"{image_dir}/{image_id:012d}.jpg\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Image ID: {image_id}\")\n",
    "        plt.axis('off')  # Ï∂ï Î≤àÌò∏ÏôÄ ÎààÍ∏àÏùÑ ÎÅïÎãàÎã§.\n",
    "        plt.show()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {image_path} not found. Please ensure the image exists in the specified directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay we can see the images. good. Lets see the yolo boxes too.   \n",
    "First, we have to get the annotations and change it to yolo boxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{558840: [[52,\n",
       "   0.3729609375,\n",
       "   0.5524590163934426,\n",
       "   0.12142187499999998,\n",
       "   0.16599531615925056],\n",
       "  [39,\n",
       "   0.5345390624999999,\n",
       "   0.36855971896955503,\n",
       "   0.052609375,\n",
       "   0.24822014051522248],\n",
       "  [39, 0.288375, 0.3169672131147541, 0.049093750000000005, 0.2077985948477752],\n",
       "  [41,\n",
       "   0.030304687500000003,\n",
       "   0.4112060889929742,\n",
       "   0.054609375,\n",
       "   0.41065573770491803],\n",
       "  [0, 0.7696640625, 0.40497658079625287, 0.456265625, 0.8049414519906323],\n",
       "  [39, 0.38909375, 0.1898946135831382, 0.0310625, 0.17969555035128806],\n",
       "  [44, 0.696640625, 0.5631850117096019, 0.11225, 0.11737704918032786],\n",
       "  [0, 0.07421875, 0.1900351288056206, 0.1454375, 0.3752927400468384],\n",
       "  [39,\n",
       "   0.45666406249999997,\n",
       "   0.10443793911007025,\n",
       "   0.049609375,\n",
       "   0.144192037470726],\n",
       "  [60, 0.2859375, 0.8012997658079625, 0.56146875, 0.36625292740046833],\n",
       "  [44,\n",
       "   0.7030234375,\n",
       "   0.5388524590163934,\n",
       "   0.11373437500000001,\n",
       "   0.1320374707259953]],\n",
       " 200365: [[52,\n",
       "   0.48267968749999995,\n",
       "   0.7008020833333333,\n",
       "   0.23342187499999997,\n",
       "   0.0803125],\n",
       "  [52, 0.4991875, 0.7849479166666666, 0.25, 0.1204375],\n",
       "  [52, 0.46432031249999994, 0.8090208333333333, 0.0016093750000000001, 0.0],\n",
       "  [52, 0.4919375, 0.7182812500000001, 0.19678125, 0.0473125],\n",
       "  [60, 0.6001875, 0.6951145833333333, 0.799625, 0.5861041666666666],\n",
       "  [1,\n",
       "   0.43937499999999996,\n",
       "   0.5373333333333333,\n",
       "   0.8787499999999999,\n",
       "   0.9244583333333334],\n",
       "  [2, 0.6024218749999999, 0.2591354166666667, 0.79515625, 0.5182708333333333],\n",
       "  [2, 0.1038828125, 0.06193749999999999, 0.204390625, 0.119375],\n",
       "  [41, 0.8016015624999999, 0.59846875, 0.155421875, 0.35502083333333334]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert COCO annotations to YOLO format\n",
    "yolo_annotations = {}\n",
    "\n",
    "for annotation in instances_data[\"annotations\"]:\n",
    "    image_id = annotation[\"image_id\"]\n",
    "    category_id = annotation[\"category_id\"]\n",
    "    bbox = annotation[\"bbox\"]\n",
    "    \n",
    "    # Convert COCO bbox (x-top-left, y-top-left, width, height) to YOLO format (x-center, y-center, width, height)\n",
    "    x_center = (bbox[0] + bbox[2] / 2) / image_id_to_dimensions[image_id][0]\n",
    "    y_center = (bbox[1] + bbox[3] / 2) / image_id_to_dimensions[image_id][1]\n",
    "    width = bbox[2] / image_id_to_dimensions[image_id][0]\n",
    "    height = bbox[3] / image_id_to_dimensions[image_id][1]\n",
    "    \n",
    "    # Get the YOLO label index\n",
    "    label_idx = category_id_to_idx[category_id]\n",
    "    \n",
    "    # Append to yolo_annotations\n",
    "    if image_id not in yolo_annotations:\n",
    "        yolo_annotations[image_id] = []\n",
    "    yolo_annotations[image_id].append([label_idx, x_center, y_center, width, height])\n",
    "\n",
    "# Display a sample of the converted annotations\n",
    "sample_annotations = {k: yolo_annotations[k] for k in list(yolo_annotations)[:2]}\n",
    "sample_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay looks like we made it well  \n",
    "Now lets make the fuc that draws the yolo boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw the bounding boxes on the image using YOLO annotations\n",
    "def draw_boxes(image, annotations, category_id_to_name):\n",
    "    import matplotlib.patches as patches\n",
    "    \n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # Image dimensions\n",
    "    im_width, im_height = image.size\n",
    "    \n",
    "    for annotation in annotations:\n",
    "        label_idx, x_center, y_center, width, height = annotation\n",
    "        \n",
    "        # Convert YOLO format (x-center, y-center, width, height) to top-left x, top-left y, width, height\n",
    "        x = (x_center - width / 2) * im_width\n",
    "        y = (y_center - height / 2) * im_height\n",
    "        box_width = width * im_width\n",
    "        box_height = height * im_height\n",
    "        \n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((x, y), box_width, box_height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        \n",
    "        # Draw rectangle on the image\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label\n",
    "        label_name = category_id_to_name[list(category_id_to_idx.keys())[label_idx]]\n",
    "        plt.text(x, y, label_name, color='white', bbox=dict(facecolor='red', edgecolor='red', pad=0))\n",
    "    \n",
    "    # Display the image\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File train2017/000000558840.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000200365.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000495357.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000116061.jpg not found. Please ensure the image exists in the specified directory.\n",
      "File train2017/000000016164.jpg not found. Please ensure the image exists in the specified directory.\n"
     ]
    }
   ],
   "source": [
    "# Display bounding boxes on the first 5 images using the yolo_annotations dictionary\n",
    "first_5_image_ids = list(yolo_annotations.keys())[:5]\n",
    "\n",
    "for image_id in first_5_image_ids:\n",
    "    image_path = f\"{image_dir}/{image_id:012d}.jpg\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        draw_boxes(image, yolo_annotations[image_id], category_id_to_name)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {image_path} not found. Please ensure the image exists in the specified directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{391895: '000000391895.jpg',\n",
       " 522418: '000000522418.jpg',\n",
       " 184613: '000000184613.jpg',\n",
       " 318219: '000000318219.jpg',\n",
       " 554625: '000000554625.jpg'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mapping of image id to its file name\n",
    "image_id_to_filename = {img[\"id\"]: img[\"file_name\"] for img in instances_data[\"images\"]}\n",
    "\n",
    "# Display a sample of the mapping\n",
    "sample_filenames = {k: image_id_to_filename[k] for k in list(image_id_to_filename)[:5]}\n",
    "sample_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yolo_annotations/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory to save the YOLO formatted annotations\n",
    "output_dir = \"yolo_annotations/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for image_id, annotations in yolo_annotations.items():\n",
    "    filename = image_id_to_filename[image_id].replace(\".jpg\", \".txt\")\n",
    "    with open(os.path.join(output_dir, filename), 'w') as file:\n",
    "        for annotation in annotations:\n",
    "            line = \" \".join(map(str, annotation))\n",
    "            file.write(line + \"\\n\")\n",
    "\n",
    "# Return the path to the directory containing the saved annotations\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay... looks good!, lets do the same thing for test , val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'val_yolo_annotations/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the instances_val2017.json file\n",
    "with open(\"annotations/instances_val2017.json\", \"r\") as file:\n",
    "    instances_data = json.load(file)\n",
    "\n",
    "image_id_to_dimensions = {img[\"id\"]: (img[\"width\"], img[\"height\"]) for img in instances_data[\"images\"]}\n",
    "\n",
    "# Convert COCO annotations to YOLO format\n",
    "val_yolo_annotations = {}\n",
    "\n",
    "for annotation in instances_data[\"annotations\"]:\n",
    "    image_id = annotation[\"image_id\"]\n",
    "    category_id = annotation[\"category_id\"]\n",
    "    bbox = annotation[\"bbox\"]\n",
    "    \n",
    "    # Convert COCO bbox (x-top-left, y-top-left, width, height) to YOLO format (x-center, y-center, width, height)\n",
    "    x_center = (bbox[0] + bbox[2] / 2) / image_id_to_dimensions[image_id][0]\n",
    "    y_center = (bbox[1] + bbox[3] / 2) / image_id_to_dimensions[image_id][1]\n",
    "    width = bbox[2] / image_id_to_dimensions[image_id][0]\n",
    "    height = bbox[3] / image_id_to_dimensions[image_id][1]\n",
    "    \n",
    "    # Get the YOLO label index\n",
    "    label_idx = category_id_to_idx[category_id]\n",
    "    \n",
    "    # Append to yolo_annotations\n",
    "    if image_id not in yolo_annotations:\n",
    "        val_yolo_annotations[image_id] = []\n",
    "    val_yolo_annotations[image_id].append([label_idx, x_center, y_center, width, height])\n",
    "\n",
    "# Create a mapping of image id to its file name\n",
    "image_id_to_filename = {img[\"id\"]: img[\"file_name\"] for img in instances_data[\"images\"]}\n",
    "\n",
    "import os\n",
    "\n",
    "# Directory to save the YOLO formatted annotations\n",
    "output_dir = \"val_yolo_annotations/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for image_id, annotations in val_yolo_annotations.items():\n",
    "    filename = image_id_to_filename[image_id].replace(\".jpg\", \".txt\")\n",
    "    with open(os.path.join(output_dir, filename), 'w') as file:\n",
    "        for annotation in annotations:\n",
    "            line = \" \".join(map(str, annotation))\n",
    "            file.write(line + \"\\n\")\n",
    "\n",
    "# Return the path to the directory containing the saved annotations\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YAML\n",
    "Now , to use the yolo5 we have to make the yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coco2017.yaml'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the .yaml file based on the provided structure and annotations\n",
    "\n",
    "# Assuming the classes are based on the COCO dataset loaded earlier\n",
    "class_names = [category[\"name\"] for category in instances_data[\"categories\"]]\n",
    "\n",
    "# Paths for the dataset\n",
    "dataset_name = \"coco2017\" \n",
    "train_images_path = f\"../{dataset_name}/images/train/\"\n",
    "val_images_path = f\"../{dataset_name}/images/val/\"\n",
    "train_labels_path = f\"../{dataset_name}/labels/train/\"\n",
    "val_labels_path = f\"../{dataset_name}/labels/val/\"\n",
    "\n",
    "# Construct the .yaml content\n",
    "yaml_content = {\n",
    "    \"names\": class_names,\n",
    "    \"nc\": len(class_names),\n",
    "    \"train\": train_images_path,\n",
    "    \"val\": val_images_path,\n",
    "    \"train_label\": train_labels_path,\n",
    "    \"val_label\": val_labels_path\n",
    "}\n",
    "\n",
    "# Save the .yaml content to a file\n",
    "yaml_filename = \"coco2017.yaml\"\n",
    "with open(yaml_filename, 'w') as yaml_file:\n",
    "    for key, value in yaml_content.items():\n",
    "        if isinstance(value, list):  # For the 'names' list\n",
    "            yaml_file.write(f\"{key}:\\n\")\n",
    "            for item in value:\n",
    "                yaml_file.write(f\"  - {item}\\n\")\n",
    "        else:\n",
    "            yaml_file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "yaml_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=./yolov5/models/yolov5s.yaml, data=./coco2017.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=mps, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "remote: Enumerating objects: 5, done.\u001b[K\n",
      "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 5 (delta 0), reused 4 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (5/5), 2.56 KiB | 262.00 KiB/s, done.\n",
      "From https://github.com/ultralytics/yolov5\n",
      "   dd10481..94e943e  master     -> origin/master\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 1 commit. Use 'git pull' or 'git clone https://github.com/ultralytics/yolov5' to update.\n",
      "fatal: cannot change to '/Volumes/·Ñâ·Ö¢': No such file or directory\n",
      "YOLOv5 üöÄ 2023-8-23 Python-3.11.4 torch-2.0.1 MPS\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from yolov5s.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Volumes/·Ñâ·Ö¢ ·Ñá·Ö©·ÜØ·ÑÖ·Ö≤·Ü∑/coco2017/./yolov5/train.py\", line 647, in <module>\n",
      "    main(opt)\n",
      "  File \"/Volumes/·Ñâ·Ö¢ ·Ñá·Ö©·ÜØ·ÑÖ·Ö≤·Ü∑/coco2017/./yolov5/train.py\", line 536, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"/Volumes/·Ñâ·Ö¢ ·Ñá·Ö©·ÜØ·ÑÖ·Ö≤·Ü∑/coco2017/./yolov5/train.py\", line 195, in train\n",
      "    train_loader, dataset = create_dataloader(train_path,\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/·Ñâ·Ö¢ ·Ñá·Ö©·ÜØ·ÑÖ·Ö≤·Ü∑/coco2017/yolov5/utils/dataloaders.py\", line 124, in create_dataloader\n",
      "    dataset = LoadImagesAndLabels(\n",
      "              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/·Ñâ·Ö¢ ·Ñá·Ö©·ÜØ·ÑÖ·Ö≤·Ü∑/coco2017/yolov5/utils/dataloaders.py\", line 491, in __init__\n",
      "    assert cache['hash'] == get_hash(self.label_files + self.im_files)  # identical hash\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/·Ñâ·Ö¢ ·Ñá·Ö©·ÜØ·ÑÖ·Ö≤·Ü∑/coco2017/yolov5/utils/dataloaders.py\", line 54, in get_hash\n",
      "    size = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # sizes\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Volumes/·Ñâ·Ö¢ ·Ñá·Ö©·ÜØ·ÑÖ·Ö≤·Ü∑/coco2017/yolov5/utils/dataloaders.py\", line 54, in <genexpr>\n",
      "    size = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # sizes\n",
      "                                                    ^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 19, in exists\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Extracting parameters for the training command using the variables we've used so far\n",
    "\n",
    "# Image size for training\n",
    "img_size = 640\n",
    "\n",
    "# Batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Path to the YAML file\n",
    "data_path = './coco2017.yaml'\n",
    "\n",
    "# Model configuration\n",
    "cfg_path = \"./yolov5/models/yolov5s.yaml\"  # Example using the small model configuration\n",
    "\n",
    "# Initial weights for training\n",
    "weights_path = \"yolov5s.pt\"\n",
    "\n",
    "# Adjust the train_command to use the relative path to train.py\n",
    "train_script_path = \"./yolov5/train.py\"\n",
    "\n",
    "# cpu,cuda,metal (apple)\n",
    "#device = \"metal\"\n",
    "device = 'mps'  \n",
    "\n",
    "# Construct the command with the relative path\n",
    "train_command = f\"python {train_script_path} --img {img_size} --batch {batch_size} --epochs {num_epochs} --data {data_path} --cfg {cfg_path} --weights {weights_path} --device {device}\"\n",
    "\n",
    "!{train_command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/owo/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 üöÄ 2023-8-9 Python-3.11.4 torch-2.0.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Use your model name here, e.g., 'yolov5s'\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform inference\n",
    "    results = model(frame)\n",
    "\n",
    "    # Render the results on the frame\n",
    "    rendered_frame = results.render()[0]\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('YOLOv5 Real-time Object Detection', rendered_frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
