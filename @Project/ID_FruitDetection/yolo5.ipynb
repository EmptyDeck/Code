{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple_49.xml',\n",
       " 'apple_75.xml',\n",
       " 'apple_61.xml',\n",
       " 'apple_75.jpg',\n",
       " 'apple_61.jpg',\n",
       " 'apple_49.jpg',\n",
       " 'orange_3.jpg',\n",
       " 'orange_3.xml',\n",
       " 'orange_28.jpg',\n",
       " 'banana_50.jpg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "relative_train_path = \"train\"\n",
    "current_dir = os.getcwd()  #Here\n",
    "train_path = os.path.join(current_dir, relative_train_path)\n",
    "\n",
    "train_files = os.listdir(train_path)\n",
    "\n",
    "train_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['banana_93.xml',\n",
       " 'banana_78.jpg',\n",
       " 'banana_87.xml',\n",
       " 'banana_93.jpg',\n",
       " 'banana_87.jpg',\n",
       " 'banana_78.xml',\n",
       " 'banana_86.jpg',\n",
       " 'banana_79.xml',\n",
       " 'banana_92.jpg',\n",
       " 'banana_79.jpg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "relative_test_path = \"test\"\n",
    "current_dir = os.getcwd()  #Here\n",
    "test_path = os.path.join(current_dir, relative_test_path)\n",
    "\n",
    "test_files = os.listdir(test_path)\n",
    "\n",
    "test_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 132/480 [00:00<00:00, 447.47it/s]libpng warning: iCCP: known incorrect sRGB profile\n",
      " 38%|███▊      | 183/480 [00:00<00:00, 412.74it/s]libpng warning: iCCP: known incorrect sRGB profile\n",
      " 91%|█████████ | 437/480 [00:00<00:00, 544.70it/s]libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "100%|██████████| 480/480 [00:00<00:00, 509.85it/s]\n",
      " 45%|████▌     | 54/120 [00:00<00:00, 523.81it/s]libpng warning: iCCP: known incorrect sRGB profile\n",
      "100%|██████████| 120/120 [00:00<00:00, 553.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(240, 60)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_yolo_format(xml_file, img_path, class_mapping):\n",
    "    \"\"\"Convert XML annotation to YOLO format annotation.\"\"\"\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Get image dimensions\n",
    "    img = cv2.imread(img_path)\n",
    "    img_height, img_width = img.shape[:2]\n",
    "\n",
    "    yolo_data = []\n",
    "\n",
    "    for obj in root.findall('object'):\n",
    "        class_name = obj.find('name').text\n",
    "        if class_name not in class_mapping:\n",
    "            continue\n",
    "\n",
    "        class_index = class_mapping[class_name]\n",
    "        bndbox = obj.find('bndbox')\n",
    "        \n",
    "        # Extract bounding box coordinates\n",
    "        xmin = float(bndbox.find('xmin').text)\n",
    "        ymin = float(bndbox.find('ymin').text)\n",
    "        xmax = float(bndbox.find('xmax').text)\n",
    "        ymax = float(bndbox.find('ymax').text)\n",
    "\n",
    "        # Convert to YOLO format\n",
    "        x_center = (xmin + xmax) / 2.0\n",
    "        y_center = (ymin + ymax) / 2.0\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "\n",
    "        # Normalize by image dimensions\n",
    "        x_center /= img_width\n",
    "        y_center /= img_height\n",
    "        width /= img_width\n",
    "        height /= img_height\n",
    "\n",
    "        yolo_data.append((class_index, x_center, y_center, width, height))\n",
    "\n",
    "    return yolo_data\n",
    "\n",
    "# Define class mapping\n",
    "classes = [\"apple\", \"banana\", \"orange\"]\n",
    "class_mapping = {class_name: idx for idx, class_name in enumerate(classes)}\n",
    "\n",
    "# Convert train dataset\n",
    "yolo_train_annotations = {}\n",
    "for file in tqdm(train_files):\n",
    "    if file.endswith(\".jpg\"):\n",
    "        img_path = os.path.join(train_path, file)\n",
    "        xml_file = os.path.join(train_path, file.replace('.jpg', '.xml'))\n",
    "\n",
    "        yolo_data = convert_to_yolo_format(xml_file, img_path, class_mapping)\n",
    "        yolo_train_annotations[img_path] = yolo_data\n",
    "\n",
    "# Convert test dataset\n",
    "yolo_test_annotations = {}\n",
    "for file in tqdm(test_files):\n",
    "    if file.endswith(\".jpg\"):\n",
    "        img_path = os.path.join(test_path, file)\n",
    "        xml_file = os.path.join(test_path, file.replace('.jpg', '.xml'))\n",
    "\n",
    "        yolo_data = convert_to_yolo_format(xml_file, img_path, class_mapping)\n",
    "        yolo_test_annotations[img_path] = yolo_data\n",
    "\n",
    "len(yolo_train_annotations), len(yolo_test_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owo/anaconda3/envs/torchenv/lib/python3.11/site-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def save_yolo_format(yolo_annotations, save_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    for img_path, annotations in yolo_annotations.items():\n",
    "        # Load the image using Pillow and convert to RGB\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Save the RGB image to the new directory\n",
    "        new_img_path = os.path.join(save_path, os.path.basename(img_path))\n",
    "        image.save(new_img_path)\n",
    "\n",
    "        # Write the annotations to a new .txt file\n",
    "        txt_path = os.path.join(save_path, os.path.basename(img_path).replace('.jpg', '.txt'))\n",
    "        with open(txt_path, 'w') as f:\n",
    "            for annotation in annotations:\n",
    "                f.write(' '.join(map(str, annotation)) + '\\n')\n",
    "\n",
    "# 훈련 및 테스트 어노테이션 저장\n",
    "yolo_dp_train_path = os.path.join(current_dir, \"yolo_dp_train\")\n",
    "yolo_dp_test_path = os.path.join(current_dir, \"yolo_dp_test\")\n",
    "\n",
    "save_yolo_format(yolo_train_annotations, yolo_dp_train_path)\n",
    "save_yolo_format(yolo_test_annotations, yolo_dp_test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = \"\"\"\n",
    "train: /Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/yolo_dp_train\n",
    "val: /Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/yolo_dp_test\n",
    "\n",
    "nc: 3  # number of classes\n",
    "names: ['apple', 'banana', 'orange']  # class names\n",
    "\"\"\"\n",
    "\n",
    "with open(\"/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/yolov5/data/data_config.yaml\", 'w') as f:\n",
    "    f.write(data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/yolov5/yolov5s.pt, cfg=/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/yolov5/models/yolov5s.yaml, data=/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/yolov5/data/data_config.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=1, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "^C\n",
      "\n",
      "YOLOv5 🚀 v7.0-207-gdf48c20 Python-3.11.4 torch-2.0.1 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"
     ]
    }
   ],
   "source": [
    "!python /Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/yolov5/train.py --img 640 --batch 16 --epochs 1 --data /Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/yolov5/data/data_config.yaml --cfg /Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/yolov5/models/yolov5s.yaml --weights /Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/yolov5/yolov5s.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/test/banana_78.jpg',\n",
       " '/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/test/banana_93.jpg',\n",
       " '/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/test/banana_87.jpg',\n",
       " '/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/test/banana_86.jpg',\n",
       " '/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/test/banana_92.jpg',\n",
       " '/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/test/banana_79.jpg',\n",
       " '/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/test/apple_89.jpg',\n",
       " '/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/test/banana_84.jpg',\n",
       " '/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/test/banana_90.jpg',\n",
       " '/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/test/banana_91.jpg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yolo_test_annotations의 키들을 출력합니다.\n",
    "list(yolo_test_annotations.keys())[:10]  # 처음 10개만 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'selected_test_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(selected_test_images)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'selected_test_images' is not defined"
     ]
    }
   ],
   "source": [
    "print(selected_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/owo/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2023-8-9 Python-3.11.4 torch-2.0.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Path.replace() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m selected_test_images \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39msample(test_images, \u001b[39m10\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[39m# 선택된 이미지에 대한 어노테이션을 추출하기 위해 테스트 이미지 경로를 조정합니다.\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m corrected_test_images \u001b[39m=\u001b[39m [img_path\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39myolo_dp_test\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m img_path \u001b[39min\u001b[39;00m selected_test_images]\n\u001b[1;32m     63\u001b[0m \u001b[39m# 선택된 이미지에 대한 어노테이션 추출\u001b[39;00m\n\u001b[1;32m     64\u001b[0m selected_annotations \u001b[39m=\u001b[39m {\n\u001b[1;32m     65\u001b[0m     \u001b[39mstr\u001b[39m(img_path): {  \n\u001b[1;32m     66\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m'\u001b[39m: [ann[:\u001b[39m4\u001b[39m] \u001b[39mfor\u001b[39;00m ann \u001b[39min\u001b[39;00m yolo_test_annotations[\u001b[39mstr\u001b[39m(img_path)]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[39mfor\u001b[39;00m img_path \u001b[39min\u001b[39;00m corrected_test_images\n\u001b[1;32m     70\u001b[0m }\n",
      "Cell \u001b[0;32mIn[12], line 61\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     58\u001b[0m selected_test_images \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39msample(test_images, \u001b[39m10\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[39m# 선택된 이미지에 대한 어노테이션을 추출하기 위해 테스트 이미지 경로를 조정합니다.\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m corrected_test_images \u001b[39m=\u001b[39m [img_path\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39myolo_dp_test\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m img_path \u001b[39min\u001b[39;00m selected_test_images]\n\u001b[1;32m     63\u001b[0m \u001b[39m# 선택된 이미지에 대한 어노테이션 추출\u001b[39;00m\n\u001b[1;32m     64\u001b[0m selected_annotations \u001b[39m=\u001b[39m {\n\u001b[1;32m     65\u001b[0m     \u001b[39mstr\u001b[39m(img_path): {  \n\u001b[1;32m     66\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m'\u001b[39m: [ann[:\u001b[39m4\u001b[39m] \u001b[39mfor\u001b[39;00m ann \u001b[39min\u001b[39;00m yolo_test_annotations[\u001b[39mstr\u001b[39m(img_path)]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[39mfor\u001b[39;00m img_path \u001b[39min\u001b[39;00m corrected_test_images\n\u001b[1;32m     70\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: Path.replace() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# 모델 로드\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5/runs/train/exp3/weights/best.pt')\n",
    "model.eval()\n",
    "\n",
    "def draw_boxes(image_path, boxes, labels=None, color='red'):\n",
    "    with Image.open(image_path) as img:\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        for i, box in enumerate(boxes):\n",
    "            xyxy = box[:4]\n",
    "            label = f\"{labels[i]}\" if labels else \"\"\n",
    "            draw.rectangle(xyxy, outline=color, width=2)\n",
    "            draw.text((xyxy[0], xyxy[1] - 10), label, fill=color)\n",
    "    return img\n",
    "\n",
    "def predict_and_show(images, annotations):\n",
    "    fig, axs = plt.subplots(3, len(images), figsize=(20, 12))\n",
    "    \n",
    "    # 원본 이미지 표시\n",
    "    for i, img_path in enumerate(images):\n",
    "        with Image.open(img_path) as im:\n",
    "            axs[0, i].imshow(im)\n",
    "            axs[0, i].set_title(\"Original\")\n",
    "            axs[0, i].axis('off')\n",
    "\n",
    "    # 원본 YOLO 어노테이션 그리기\n",
    "    for i, img_path in enumerate(images):\n",
    "        boxes = annotations[img_path]['boxes']\n",
    "        labels = annotations[img_path]['labels']\n",
    "        img = draw_boxes(img_path, boxes, labels)\n",
    "        axs[1, i].imshow(img)\n",
    "        axs[1, i].set_title(\"Original YOLO Annotations\")\n",
    "        axs[1, i].axis('off')\n",
    "\n",
    "    # 예측된 바운딩 박스 그리기\n",
    "    results = model(images)\n",
    "    for i, img_path in enumerate(images):\n",
    "        pred_boxes = results.pred[i][:, :-1].tolist()\n",
    "        pred_labels = [model.names[int(cls)] for cls in results.pred[i][:, -1]]\n",
    "        img = draw_boxes(img_path, pred_boxes, pred_labels)\n",
    "        axs[2, i].imshow(img)\n",
    "        axs[2, i].set_title(\"Predicted\")\n",
    "        axs[2, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 이미지 경로를 Path 객체로 가져옵니다.\n",
    "test_images = [Path(p) for p in os.listdir(yolo_dp_test_path) if p.endswith('.jpg')]\n",
    "\n",
    "# 테스트 이미지 중에서 무작위로 10개를 선택합니다.\n",
    "selected_test_images = random.sample(test_images, 10)\n",
    "\n",
    "# 선택된 이미지에 대한 어노테이션을 추출하기 위해 테스트 이미지 경로를 조정합니다.\n",
    "corrected_test_images = [img_path.replace(\"yolo_dp_test\", \"test\") for img_path in selected_test_images]\n",
    "\n",
    "# 선택된 이미지에 대한 어노테이션 추출\n",
    "selected_annotations = {\n",
    "    str(img_path): {  \n",
    "        'boxes': [ann[:4] for ann in yolo_test_annotations[str(img_path)]],\n",
    "        'labels': [model.names[int(ann[4])] for ann in yolo_test_annotations[str(img_path)]]\n",
    "    }\n",
    "    for img_path in corrected_test_images\n",
    "}\n",
    "\n",
    "# 예측 및 결과 출력\n",
    "predict_and_show(corrected_test_images, selected_annotations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
