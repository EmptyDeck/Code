{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple_49.xml',\n",
       " 'apple_75.xml',\n",
       " 'apple_61.xml',\n",
       " 'apple_75.jpg',\n",
       " 'apple_61.jpg',\n",
       " 'apple_49.jpg',\n",
       " 'orange_3.jpg',\n",
       " 'orange_3.xml',\n",
       " 'orange_28.jpg',\n",
       " 'banana_50.jpg']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "relative_train_path = \"train\"\n",
    "current_dir = os.getcwd()  #Here\n",
    "train_path = os.path.join(current_dir, relative_train_path)\n",
    "\n",
    "train_files = os.listdir(train_path)\n",
    "\n",
    "train_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['banana_93.xml',\n",
       " 'banana_78.jpg',\n",
       " 'banana_87.xml',\n",
       " 'banana_93.jpg',\n",
       " 'banana_87.jpg',\n",
       " 'banana_78.xml',\n",
       " 'banana_86.jpg',\n",
       " 'banana_79.xml',\n",
       " 'banana_92.jpg',\n",
       " 'banana_79.jpg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "relative_test_path = \"test\"\n",
    "current_dir = os.getcwd()  #Here\n",
    "test_path = os.path.join(current_dir, relative_test_path)\n",
    "\n",
    "test_files = os.listdir(test_path)\n",
    "\n",
    "test_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<annotation>\n",
      "\t<folder>train</folder>\n",
      "\t<filename>apple_28.jpg</filename>\n",
      "\t<path>C:\\tensorflow1\\models\\research\\object_detection\\images\\train\\apple_28.jpg</path>\n",
      "\t<source>\n",
      "\t\t<database>Unknown</database>\n",
      "\t</source>\n",
      "\t<size>\n",
      "\t\t<width>0</width>\n",
      "\t\t<height>0</height>\n",
      "\t\t<depth>3</depth>\n",
      "\t</size>\n",
      "\t<segmented>0</segmented>\n",
      "\t<object>\n",
      "\t\t<name>apple</name>\n",
      "\t\t<pose>Unspecified</pose>\n",
      "\t\t<truncated>0</truncated>\n",
      "\t\t<difficult>0</difficult>\n",
      "\t\t<bndbox>\n",
      "\t\t\t<xmin>25</xmin>\n",
      "\t\t\t<ymin>42</ymin>\n",
      "\t\t\t<xmax>275</xmax>\n",
      "\t\t\t<ymax>297</ymax>\n",
      "\t\t</bndbox>\n",
      "\t</object>\n",
      "</annotation>\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Select a random XML file\n",
    "xml_file = os.path.join(train_path, 'apple_28.xml')\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse(xml_file)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Print the entire XML content\n",
    "ET.dump(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "import glob\n",
    "\n",
    "# Create a dictionary to map class names to integer IDs\n",
    "class_dict = {\"apple\": 0, \"banana\": 1, \"mixed\": 2, \"orange\": 3}\n",
    "\n",
    "def parse_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    boxes = []\n",
    "    labels = []\n",
    "\n",
    "    for obj in root.iter(\"object\"):\n",
    "        label = class_dict[obj.find(\"name\").text]\n",
    "\n",
    "        bbox = obj.find(\"bndbox\")\n",
    "        xmin = int(bbox.find(\"xmin\").text)\n",
    "        ymin = int(bbox.find(\"ymin\").text)\n",
    "        xmax = int(bbox.find(\"xmax\").text)\n",
    "        ymax = int(bbox.find(\"ymax\").text)\n",
    "\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        labels.append(label)\n",
    "\n",
    "    return {\"boxes\": boxes, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 60)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FruitDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "        self.imgs = sorted(glob.glob(os.path.join(root, \"*.jpg\")))\n",
    "        self.labels = sorted(glob.glob(os.path.join(root, \"*.xml\")))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "        label_path = self.labels[idx]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Parse the XML file\n",
    "        label_data = parse_xml(label_path)\n",
    "\n",
    "        # Convert the bounding boxes and labels to tensors\n",
    "        boxes = torch.as_tensor(label_data[\"boxes\"], dtype=torch.float32)\n",
    "        labels = torch.as_tensor(label_data[\"labels\"], dtype=torch.int64)\n",
    "\n",
    "        # Compute the area of the bounding boxes\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # All instances are not crowd\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transform:\n",
    "            img, target = self.transform(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "# Create instances of the FruitDataset\n",
    "train_dataset = FruitDataset(train_path)\n",
    "test_dataset = FruitDataset(test_path)\n",
    "\n",
    "# Number of images in the training and test sets\n",
    "len(train_dataset), len(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owo/anaconda3/envs/torchenv/lib/python3.11/site-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 9, 1, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of fruits per image in the training set\n",
    "num_fruits_train = [len(data[1][\"labels\"]) for data in train_dataset]\n",
    "\n",
    "# Calculate the number of fruits per image in the test set\n",
    "num_fruits_test = [len(data[1][\"labels\"]) for data in test_dataset]\n",
    "\n",
    "# Compute some statistics\n",
    "min_fruits_train, max_fruits_train = min(num_fruits_train), max(num_fruits_train)\n",
    "min_fruits_test, max_fruits_test = min(num_fruits_test), max(num_fruits_test)\n",
    "\n",
    "min_fruits_train, max_fruits_train, min_fruits_test, max_fruits_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image, target):\n",
    "    # Resize the image and target\n",
    "    resize = T.Resize((416, 416))\n",
    "    image = resize(image)\n",
    "\n",
    "    new_target = target.copy()\n",
    "    new_target[\"boxes\"] = target[\"boxes\"] / torch.tensor([image.width, image.height, image.width, image.height])\n",
    "\n",
    "    # Convert the bounding boxes to YOLO format\n",
    "    new_target[\"boxes\"] = torch.stack([\n",
    "        torch.max(new_target[\"boxes\"][:, 0], new_target[\"boxes\"][:, 2]),  # x_center\n",
    "        torch.max(new_target[\"boxes\"][:, 1], new_target[\"boxes\"][:, 3]),  # y_center\n",
    "        torch.abs(new_target[\"boxes\"][:, 2] - new_target[\"boxes\"][:, 0]),  # width\n",
    "        torch.abs(new_target[\"boxes\"][:, 3] - new_target[\"boxes\"][:, 1])  # height\n",
    "    ], dim=1)\n",
    "\n",
    "    # Normalize the image\n",
    "    normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    image = normalize(T.ToTensor()(image))\n",
    "\n",
    "    return image, new_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image, target):\n",
    "    # Resize the image and target\n",
    "    resize = T.Resize((416, 416))\n",
    "    image = resize(image)\n",
    "\n",
    "    new_target = target.copy()\n",
    "    new_target[\"boxes\"] = target[\"boxes\"] / torch.tensor([image.width, image.height, image.width, image.height])\n",
    "\n",
    "    # Convert the bounding boxes to YOLO format\n",
    "    new_target[\"boxes\"] = torch.stack([\n",
    "        (new_target[\"boxes\"][:, 0] + new_target[\"boxes\"][:, 2]) / 2,  # x_center\n",
    "        (new_target[\"boxes\"][:, 1] + new_target[\"boxes\"][:, 3]) / 2,  # y_center\n",
    "        new_target[\"boxes\"][:, 2] - new_target[\"boxes\"][:, 0],  # width\n",
    "        new_target[\"boxes\"][:, 3] - new_target[\"boxes\"][:, 1]  # height\n",
    "    ], dim=1)\n",
    "\n",
    "    # Normalize the image\n",
    "    normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    image = normalize(T.ToTensor()(image))\n",
    "\n",
    "    return image, new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 416, 416]),\n",
       " {'boxes': tensor([[0.4075, 0.4375, 0.7764, 0.8029]]),\n",
       "  'labels': tensor([0]),\n",
       "  'image_id': tensor([0]),\n",
       "  'area': tensor([107882.]),\n",
       "  'iscrowd': tensor([0])})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create instances of the FruitDataset with the transform\n",
    "train_dataset = FruitDataset(train_path, transform=transform)\n",
    "test_dataset = FruitDataset(test_path, transform=transform)\n",
    "\n",
    "# Check the transformation\n",
    "img, target = train_dataset[0]\n",
    "img.shape, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[[ 1.1187,  1.0673,  0.9646,  ..., -0.5424, -0.4226, -0.4739],\n",
       "           [ 1.1015,  1.0331,  0.9303,  ..., -0.2684, -0.0801, -0.0972],\n",
       "           [ 1.1015,  1.0159,  0.8961,  ...,  0.0227,  0.2453,  0.2453],\n",
       "           ...,\n",
       "           [ 2.2318,  2.2318,  2.2489,  ...,  2.2318,  2.2318,  2.2318],\n",
       "           [ 2.2147,  2.2147,  2.2318,  ...,  2.2318,  2.2318,  2.2318],\n",
       "           [ 2.1975,  2.2318,  2.2489,  ...,  2.2318,  2.2318,  2.2318]],\n",
       "  \n",
       "          [[ 1.7283,  1.6933,  1.6583,  ..., -0.2325, -0.1099, -0.1625],\n",
       "           [ 1.7108,  1.6758,  1.6232,  ...,  0.0826,  0.2577,  0.2227],\n",
       "           [ 1.6933,  1.6583,  1.5882,  ...,  0.3978,  0.6078,  0.5378],\n",
       "           ...,\n",
       "           [ 0.1527,  0.1877,  0.2577,  ...,  1.2556,  1.2031,  1.1506],\n",
       "           [ 0.0826,  0.1176,  0.1877,  ...,  1.2556,  1.2031,  1.1506],\n",
       "           [ 0.0476,  0.1176,  0.1702,  ...,  1.2556,  1.2031,  1.1506]],\n",
       "  \n",
       "          [[-0.2881, -0.3404, -0.4450,  ..., -1.2293, -1.2467, -1.3339],\n",
       "           [-0.3055, -0.3927, -0.4973,  ..., -1.0550, -1.0376, -1.1421],\n",
       "           [-0.3404, -0.4275, -0.5670,  ..., -0.9156, -0.9156, -1.0550],\n",
       "           ...,\n",
       "           [-0.8807, -0.8633, -0.8110,  ..., -1.7870, -1.7870, -1.7870],\n",
       "           [-0.9678, -0.9330, -0.8807,  ..., -1.7870, -1.7870, -1.7870],\n",
       "           [-1.0027, -0.9504, -0.9156,  ..., -1.7870, -1.7870, -1.7870]]]),\n",
       "  tensor([[[2.2147, 2.2147, 2.2147,  ..., 2.2489, 2.2318, 2.2147],\n",
       "           [2.2147, 2.2147, 2.2147,  ..., 2.2489, 2.2318, 2.2147],\n",
       "           [2.2147, 2.2147, 2.2147,  ..., 2.2489, 2.2318, 2.2147],\n",
       "           ...,\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
       "  \n",
       "          [[2.3235, 2.3235, 2.3235,  ..., 2.3410, 2.3235, 2.3060],\n",
       "           [2.3235, 2.3235, 2.3235,  ..., 2.3410, 2.3235, 2.3060],\n",
       "           [2.3235, 2.3235, 2.3235,  ..., 2.3410, 2.3235, 2.3060],\n",
       "           ...,\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
       "  \n",
       "          [[2.5529, 2.5529, 2.5529,  ..., 2.6400, 2.6400, 2.6226],\n",
       "           [2.5529, 2.5529, 2.5529,  ..., 2.6400, 2.6400, 2.6226],\n",
       "           [2.5529, 2.5529, 2.5529,  ..., 2.6400, 2.6400, 2.6226],\n",
       "           ...,\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]]),\n",
       "  tensor([[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           ...,\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
       "  \n",
       "          [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           ...,\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
       "  \n",
       "          [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           ...,\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]]),\n",
       "  tensor([[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           ...,\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
       "  \n",
       "          [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           ...,\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
       "  \n",
       "          [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           ...,\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]]),\n",
       "  tensor([[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           ...,\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
       "  \n",
       "          [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           ...,\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
       "  \n",
       "          [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           ...,\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]]),\n",
       "  tensor([[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           ...,\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
       "  \n",
       "          [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           ...,\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
       "  \n",
       "          [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           ...,\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]]),\n",
       "  tensor([[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           ...,\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "           [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
       "  \n",
       "          [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           ...,\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "           [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
       "  \n",
       "          [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           ...,\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "           [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]]),\n",
       "  tensor([[[2.0434, 2.0434, 2.0434,  ..., 2.0434, 2.0434, 2.0434],\n",
       "           [2.0263, 2.0263, 2.0263,  ..., 2.0434, 2.0434, 2.0434],\n",
       "           [2.0092, 2.0092, 2.0092,  ..., 2.0434, 2.0434, 2.0434],\n",
       "           ...,\n",
       "           [2.0092, 2.0092, 2.0092,  ..., 1.9920, 1.9920, 1.9920],\n",
       "           [1.9920, 1.9920, 1.9920,  ..., 1.9920, 1.9920, 1.9920],\n",
       "           [2.0092, 2.0092, 2.0092,  ..., 1.9920, 1.9920, 1.9920]],\n",
       "  \n",
       "          [[2.2185, 2.2185, 2.2185,  ..., 2.2185, 2.2185, 2.2185],\n",
       "           [2.2010, 2.2010, 2.2010,  ..., 2.2185, 2.2185, 2.2185],\n",
       "           [2.1835, 2.1835, 2.1835,  ..., 2.2185, 2.2185, 2.2185],\n",
       "           ...,\n",
       "           [2.1835, 2.1835, 2.1835,  ..., 2.1660, 2.1660, 2.1660],\n",
       "           [2.1660, 2.1660, 2.1660,  ..., 2.1660, 2.1660, 2.1660],\n",
       "           [2.1835, 2.1835, 2.1835,  ..., 2.1660, 2.1660, 2.1660]],\n",
       "  \n",
       "          [[2.3960, 2.3960, 2.3960,  ..., 2.3960, 2.3960, 2.3960],\n",
       "           [2.3786, 2.3786, 2.3786,  ..., 2.3960, 2.3960, 2.3960],\n",
       "           [2.3611, 2.3611, 2.3611,  ..., 2.3960, 2.3960, 2.3960],\n",
       "           ...,\n",
       "           [2.3611, 2.3611, 2.3611,  ..., 2.3437, 2.3437, 2.3437],\n",
       "           [2.3437, 2.3437, 2.3437,  ..., 2.3437, 2.3437, 2.3437],\n",
       "           [2.3611, 2.3611, 2.3611,  ..., 2.3437, 2.3437, 2.3437]]])),\n",
       " ({'boxes': tensor([[1.9675, 0.8089, 1.0168, 0.9784],\n",
       "           [3.0433, 1.0661, 0.9231, 1.0264],\n",
       "           [3.0240, 2.0000, 1.1538, 1.0385]]),\n",
       "   'labels': tensor([3, 3, 3]),\n",
       "   'image_id': tensor([181]),\n",
       "   'area': tensor([172161., 163968., 207360.]),\n",
       "   'iscrowd': tensor([0, 0, 0])},\n",
       "  {'boxes': tensor([[2.3654, 1.3810, 0.9087, 0.9880],\n",
       "           [1.2212, 0.6298, 1.7356, 1.0288],\n",
       "           [1.2115, 1.1755, 1.6106, 1.4856]]),\n",
       "   'labels': tensor([3, 1, 1]),\n",
       "   'image_id': tensor([159]),\n",
       "   'area': tensor([155358., 309016., 414060.]),\n",
       "   'iscrowd': tensor([0, 0, 0])},\n",
       "  {'boxes': tensor([[1.2945, 0.6538, 1.1274, 0.8029],\n",
       "           [0.5745, 0.5757, 1.1346, 0.9591]]),\n",
       "   'labels': tensor([0, 0]),\n",
       "   'image_id': tensor([24]),\n",
       "   'area': tensor([156646., 188328.]),\n",
       "   'iscrowd': tensor([0, 0])},\n",
       "  {'boxes': tensor([[0.4038, 0.4303, 0.5048, 0.6298]]),\n",
       "   'labels': tensor([1]),\n",
       "   'image_id': tensor([99]),\n",
       "   'area': tensor([55020.]),\n",
       "   'iscrowd': tensor([0])},\n",
       "  {'boxes': tensor([[1.0781, 0.7572, 1.5553, 0.8750]]),\n",
       "   'labels': tensor([1]),\n",
       "   'image_id': tensor([108]),\n",
       "   'area': tensor([235508.]),\n",
       "   'iscrowd': tensor([0])},\n",
       "  {'boxes': tensor([[0.4159, 0.4531, 0.5481, 0.5601]]),\n",
       "   'labels': tensor([3]),\n",
       "   'image_id': tensor([207]),\n",
       "   'area': tensor([53124.]),\n",
       "   'iscrowd': tensor([0])},\n",
       "  {'boxes': tensor([[0.5805, 0.7572, 0.4543, 0.4663],\n",
       "           [0.4363, 0.5373, 0.3966, 0.4447],\n",
       "           [0.7680, 0.5601, 0.4062, 0.4375]]),\n",
       "   'labels': tensor([0, 0, 0]),\n",
       "   'image_id': tensor([15]),\n",
       "   'area': tensor([36666., 30525., 30758.]),\n",
       "   'iscrowd': tensor([0, 0, 0])},\n",
       "  {'boxes': tensor([[1.8101, 0.8858, 0.9471, 0.9159],\n",
       "           [0.6575, 0.8582, 0.9639, 0.9279],\n",
       "           [1.2440, 1.8870, 1.6370, 0.8702]]),\n",
       "   'labels': tensor([3, 0, 1]),\n",
       "   'image_id': tensor([163]),\n",
       "   'area': tensor([150114., 154786., 246522.]),\n",
       "   'iscrowd': tensor([0, 0, 0])}))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Test the data loading\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owo/anaconda3/envs/torchenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/owo/anaconda3/envs/torchenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "num_classes = len(class_dict) + 1\n",
    "\n",
    "# Load a pre-trained version of the model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Get the number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# Replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "\n",
    "device = 'mps' ## USE cpu or cuda if error\n",
    "\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Define the training function\n",
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, targets in data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        total_loss += losses.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Get the predicted classes\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Compare with the targets\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_one_epoch() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     12\u001b[0m     \u001b[39m# Train for one epoch\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     train_one_epoch(model, optimizer, train_loader, device, epoch)\n\u001b[1;32m     14\u001b[0m     \u001b[39m# Update the learning rate\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     lr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "\u001b[0;31mTypeError\u001b[0m: train_one_epoch() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the training parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Train the model for 10 epochs\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Train for one epoch\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "    # Update the learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "\n",
    "    # Evaluate on the test dataset\n",
    "    evaluate(model, test_loader, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "All bounding boxes should have positive height and width. Found invalid box [1.911520004272461, 0.9384245276451111, 1.3914570808410645, 1.4792898893356323] for target at index 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 42\u001b[0m     train_loss \u001b[39m=\u001b[39m train_one_epoch(model, optimizer, train_loader, device)\n\u001b[1;32m     43\u001b[0m     accuracy \u001b[39m=\u001b[39m evaluate(model, test_loader, device)\n\u001b[1;32m     45\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m, Accuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[41], line 10\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m images \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(image\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images)\n\u001b[1;32m      8\u001b[0m targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets]\n\u001b[0;32m---> 10\u001b[0m loss_dict \u001b[39m=\u001b[39m model(images, targets)\n\u001b[1;32m     11\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torchvision/models/detection/generalized_rcnn.py:95\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     93\u001b[0m             bb_idx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(degenerate_boxes\u001b[39m.\u001b[39many(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m     94\u001b[0m             degen_bb: List[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m boxes[bb_idx]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m---> 95\u001b[0m             torch\u001b[39m.\u001b[39m_assert(\n\u001b[1;32m     96\u001b[0m                 \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     97\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAll bounding boxes should have positive height and width.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Found invalid box \u001b[39m\u001b[39m{\u001b[39;00mdegen_bb\u001b[39m}\u001b[39;00m\u001b[39m for target at index \u001b[39m\u001b[39m{\u001b[39;00mtarget_idx\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     99\u001b[0m             )\n\u001b[1;32m    101\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone(images\u001b[39m.\u001b[39mtensors)\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.11/site-packages/torch/__init__.py:1209\u001b[0m, in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(condition) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mTensor \u001b[39mand\u001b[39;00m has_torch_function((condition,)):\n\u001b[1;32m   1208\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(_assert, (condition,), condition, message)\n\u001b[0;32m-> 1209\u001b[0m \u001b[39massert\u001b[39;00m condition, message\n",
      "\u001b[0;31mAssertionError\u001b[0m: All bounding boxes should have positive height and width. Found invalid box [1.911520004272461, 0.9384245276451111, 1.3914570808410645, 1.4792898893356323] for target at index 0."
     ]
    }
   ],
   "source": [
    "# Define the training function\n",
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for images, targets in data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += losses.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Define the evaluation function\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for images, targets in data_loader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "# Training the model for 10 epochs\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, optimizer, train_loader, device)\n",
    "    accuracy = evaluate(model, test_loader, device)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
