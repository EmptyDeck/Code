{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import necessary libraries and modules\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "relative_train_path = \"train\"\n",
    "current_dir = os.getcwd()  #Here\n",
    "train_path = os.path.join(current_dir, relative_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/train/apple_75.jpg',\n",
       " '/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/train/apple_61.jpg',\n",
       " '/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/train/apple_49.jpg',\n",
       " '/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/train/orange_3.jpg',\n",
       " '/Users/owo/HOUSE/@Code/@Project/ID_FruitDetection/train/orange_28.jpg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_paths = [os.path.join(train_path, filename) for filename in os.listdir(train_path) if filename.endswith('.jpg')]\n",
    "train_label_paths = [path.replace('.jpg', '.xml') for path in train_image_paths]\n",
    "train_image_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Convert XML annotations to YOLO format\n",
    "\n",
    "def convert_coordinates(size, box):\n",
    "    dw = 1. / size[0]\n",
    "    dh = 1. / size[1]\n",
    "    x = (box[0] + box[1]) / 2.0\n",
    "    y = (box[2] + box[3]) / 2.0\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    x = x * dw\n",
    "    w = w * dw\n",
    "    y = y * dh\n",
    "    h = h * dh\n",
    "    return [x, y, w, h]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_annotation(annotation_path, class_dict):\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    size = root.find('size')\n",
    "    w = int(size.find('width').text)\n",
    "    h = int(size.find('height').text)\n",
    "\n",
    "    yolo_annotations = []\n",
    "    for obj in root.iter('object'):\n",
    "        difficult = obj.find('difficult').text\n",
    "        cls = obj.find('name').text\n",
    "        if cls not in class_dict or int(difficult) == 1:\n",
    "            continue\n",
    "        cls_id = class_dict[cls]\n",
    "        xmlbox = obj.find('bndbox')\n",
    "        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text),\n",
    "             float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))\n",
    "        converted_coordinates = convert_coordinates((w, h), b)\n",
    "        yolo_annotations.append([cls_id, *converted_coordinates])\n",
    "    \n",
    "    return yolo_annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {\"apple\": 0, \"banana\": 1, \"orange\": 2, \"mixed\": 3}\n",
    "\n",
    "# Convert XML annotations to YOLO format and save them\n",
    "yolo_train_annotations = [convert_annotation(path, class_dict) for path in train_label_paths]\n",
    "train_annotation_save_paths = [path.replace('.xml', '.txt') for path in train_label_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for annotations, save_path in zip(yolo_train_annotations, train_annotation_save_paths):\n",
    "    np.savetxt(save_path, annotations, fmt=[\"%d\", \"%f\", \"%f\", \"%f\", \"%f\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loader and Transformations\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((416, 416)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = YOLODataset(train_image_paths, train_annotation_save_paths, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# 2. Define the Simplified YOLO Architecture\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SimpleYOLO(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleYOLO, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        grid_size = 416 // 8\n",
    "        self.detector = nn.Conv2d(128, 5 + num_classes, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.detector(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleYOLO(num_classes)\n",
    "\n",
    "\n",
    "# 3. Loss Function and Optimizer\n",
    "\n",
    "yolo_loss = YOLOLoss(num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# 4. Training the Model\n",
    "\n",
    "num_epochs = 5\n",
    "model.train()\n",
    "model.to(device)\n",
    "yolo_loss.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for images, labels_list in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        labels = [labels[0] for labels in labels_list]\n",
    "        labels = torch.stack(labels).to(device)\n",
    "        loss = yolo_loss(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training process summarized.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
